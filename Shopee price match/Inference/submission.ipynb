{
 "cells": [
  {
   "cell_type": "code",
   "id": "played-judge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:00.391904Z",
     "iopub.status.busy": "2021-05-10T14:27:00.391136Z",
     "iopub.status.idle": "2021-05-10T14:27:11.814391Z",
     "shell.execute_reply": "2021-05-10T14:27:11.813815Z"
    },
    "papermill": {
     "duration": 11.474014,
     "end_time": "2021-05-10T14:27:11.814550",
     "exception": false,
     "start_time": "2021-05-10T14:27:00.340536",
     "status": "completed"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('../input/shopee-pytorch-models')\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "import random \n",
    "import os \n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import albumentations as A \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import gc\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "#--------------\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "\n",
    "import transformers\n",
    "from transformers import (BertTokenizer, BertModel,\n",
    "                          DistilBertTokenizer, DistilBertModel,\n",
    "                          RobertaTokenizer, RobertaModel,\n",
    "                          AutoTokenizer, AutoModel)\n",
    "\n",
    "from cuml import PCA\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cutting-marble",
   "metadata": {
    "papermill": {
     "duration": 0.042531,
     "end_time": "2021-05-10T14:27:11.900304",
     "exception": false,
     "start_time": "2021-05-10T14:27:11.857773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imagenet-Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "municipal-gibraltar",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:11.992578Z",
     "iopub.status.busy": "2021-05-10T14:27:11.991894Z",
     "iopub.status.idle": "2021-05-10T14:27:20.719632Z",
     "shell.execute_reply": "2021-05-10T14:27:20.718648Z"
    },
    "papermill": {
     "duration": 8.776152,
     "end_time": "2021-05-10T14:27:20.719886",
     "exception": false,
     "start_time": "2021-05-10T14:27:11.943734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "GET_CV = False #True # kaggle commit할때는 False로 지정\n",
    "CHECK_SUB = False\n",
    "\n",
    "df = cudf.read_csv('../input/shopee-product-matching/test.csv')\n",
    "# If we are comitting, replace train set for test set and dont get cv\n",
    "if len(df) > 3:\n",
    "    GET_CV = False\n",
    "del df\n",
    "\n",
    "print(GET_CV)\n",
    "\n",
    "class CFG:\n",
    "    img_size = 512\n",
    "    batch_size = 12\n",
    "    seed = 2020\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes = 11014\n",
    "    \n",
    "    model_name1 = 'eca_nfnet_l1'\n",
    "    model_path1 = '../input/nf-45epochs/Curr_Arc_512x512_eca_nfnet_l1(mish)_45EpochStep_adamw.pt'\n",
    "    \n",
    "    model_name2 = 'efficientnet_b3'\n",
    "    model_path2 = '../input/effb3-cur-arc-30epoch-weight/Curr_Arc_512x512_efficientnet_b3(mish)_29EpochStep_adamw (1).pt'\n",
    "                   \n",
    "    model_name3 = 'eca_nfnet_l0'\n",
    "    model_path3 = '../input/curr-arc-nfnet-weight/Curr_Arc_512x512_nfnet_l0(mish)_29EpochStep_adamw.pt'\n",
    "    \n",
    "    # model_name4 = 'eca_nfnet_l0'\n",
    "    # model_path4 = '../input/curr-arc-nfnet-weight/Curr_Arc_512x512_nfnet_l0(mish)_29EpochStep_adamw.pt'\n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ethical-variety",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:20.812118Z",
     "iopub.status.busy": "2021-05-10T14:27:20.811134Z",
     "iopub.status.idle": "2021-05-10T14:27:20.815091Z",
     "shell.execute_reply": "2021-05-10T14:27:20.814551Z"
    },
    "papermill": {
     "duration": 0.051625,
     "end_time": "2021-05-10T14:27:20.815227",
     "exception": false,
     "start_time": "2021-05-10T14:27:20.763602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG2:\n",
    "    bert_hidden_size = 768 #Bert-base\n",
    "    SEED = 42\n",
    "    batch_size = 16 #64 #32?\n",
    "    num_workers = 4\n",
    "    max_length = 30\n",
    "    device = \"cuda\"\n",
    "    NUM_CLASSES = 11014\n",
    "    TEXT_MODEL_PATH1 = '../input/robertdistilbertweights/model_100epochs.pt'    \n",
    "    TEXT_MODEL_PATH2 = '../input/wholeroberta/whole_roberta_150epochs.pt_147'\n",
    "    TEXT_MODEL_PATH3 = '../input/distilmerge-20epochs/Mergeface_adamw_20epochStep_model_100epochs.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unauthorized-basis",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:20.906081Z",
     "iopub.status.busy": "2021-05-10T14:27:20.905187Z",
     "iopub.status.idle": "2021-05-10T14:27:20.909108Z",
     "shell.execute_reply": "2021-05-10T14:27:20.908531Z"
    },
    "papermill": {
     "duration": 0.052405,
     "end_time": "2021-05-10T14:27:20.909237",
     "exception": false,
     "start_time": "2021-05-10T14:27:20.856832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "younger-dodge",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.001595Z",
     "iopub.status.busy": "2021-05-10T14:27:21.000718Z",
     "iopub.status.idle": "2021-05-10T14:27:21.005070Z",
     "shell.execute_reply": "2021-05-10T14:27:21.004460Z"
    },
    "papermill": {
     "duration": 0.053477,
     "end_time": "2021-05-10T14:27:21.005211",
     "exception": false,
     "start_time": "2021-05-10T14:27:20.951734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/train_images/' + df['image']\n",
    "    else:\n",
    "        df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        image_paths = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "        \n",
    "    return df, df_cu, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "numerical-uniform",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.097546Z",
     "iopub.status.busy": "2021-05-10T14:27:21.096704Z",
     "iopub.status.idle": "2021-05-10T14:27:21.105035Z",
     "shell.execute_reply": "2021-05-10T14:27:21.104454Z"
    },
    "papermill": {
     "duration": 0.056728,
     "end_time": "2021-05-10T14:27:21.105173",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.048445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-concord",
   "metadata": {
    "papermill": {
     "duration": 0.042113,
     "end_time": "2021-05-10T14:27:21.190576",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.148463",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pacific-viking",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.283888Z",
     "iopub.status.busy": "2021-05-10T14:27:21.282872Z",
     "iopub.status.idle": "2021-05-10T14:27:21.286614Z",
     "shell.execute_reply": "2021-05-10T14:27:21.286093Z"
    },
    "papermill": {
     "duration": 0.051944,
     "end_time": "2021-05-10T14:27:21.286774",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.234830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n",
    "            A.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-limit",
   "metadata": {
    "papermill": {
     "duration": 0.046073,
     "end_time": "2021-05-10T14:27:21.376621",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.330548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imagenet-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "collectible-slovak",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.484135Z",
     "iopub.status.busy": "2021-05-10T14:27:21.481500Z",
     "iopub.status.idle": "2021-05-10T14:27:21.485113Z",
     "shell.execute_reply": "2021-05-10T14:27:21.485596Z"
    },
    "papermill": {
     "duration": 0.055743,
     "end_time": "2021-05-10T14:27:21.485777",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.430034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-suicide",
   "metadata": {
    "papermill": {
     "duration": 0.042562,
     "end_time": "2021-05-10T14:27:21.570920",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.528358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pacific-fault",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.665408Z",
     "iopub.status.busy": "2021-05-10T14:27:21.664521Z",
     "iopub.status.idle": "2021-05-10T14:27:21.668449Z",
     "shell.execute_reply": "2021-05-10T14:27:21.667922Z"
    },
    "papermill": {
     "duration": 0.054798,
     "end_time": "2021-05-10T14:27:21.668597",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.613799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeDataset2(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, mode=\"test\", max_length=None):\n",
    "        self.dataframe = dataframe\n",
    "        if mode != \"test\":\n",
    "            self.targets = dataframe['label_code'].values\n",
    "        texts = list(dataframe['title'].apply(lambda o: str(o)).values)\n",
    "        self.encodings = tokenizer(texts, \n",
    "                                   padding=True, \n",
    "                                   truncation=True, \n",
    "                                   max_length=max_length)\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}\n",
    "        if self.mode != \"test\":\n",
    "            item['labels'] = torch.tensor(self.targets[idx]).long()\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-bronze",
   "metadata": {
    "papermill": {
     "duration": 0.042444,
     "end_time": "2021-05-10T14:27:21.753153",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.710709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Curr + Arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mature-lodging",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:21.859275Z",
     "iopub.status.busy": "2021-05-10T14:27:21.847893Z",
     "iopub.status.idle": "2021-05-10T14:27:21.869840Z",
     "shell.execute_reply": "2021-05-10T14:27:21.870482Z"
    },
    "papermill": {
     "duration": 0.074898,
     "end_time": "2021-05-10T14:27:21.870632",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.795734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "credit : https://github.com/HuangYG123/CurricularFace/blob/8b2f47318117995aa05490c05b455b113489917e/head/metrics.py#L70\n",
    "'''\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        \n",
    "        print('Using ArcFace')\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda:1')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "        return output#, nn.CrossEntropyLoss()(output,label)\n",
    "\n",
    "\n",
    "\n",
    "def l2_norm(input, axis = 1):\n",
    "    norm = torch.norm(input, 2, axis, True)\n",
    "    output = torch.div(input, norm)\n",
    "\n",
    "    return output\n",
    "\n",
    "class CurricularFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, s = 30, m = 0.50):\n",
    "        super(CurricularFace, self).__init__()\n",
    "\n",
    "        print('Using Curricular Face')\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.kernel = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        self.register_buffer('t', torch.zeros(1))\n",
    "        nn.init.normal_(self.kernel, std=0.01)\n",
    "\n",
    "    def forward(self, embbedings, label):\n",
    "        embbedings = l2_norm(embbedings, axis = 1)\n",
    "        kernel_norm = l2_norm(self.kernel, axis = 0)\n",
    "        cos_theta = torch.mm(embbedings, kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1, 1)  # for numerical stability\n",
    "        with torch.no_grad():\n",
    "            origin_cos = cos_theta.clone()\n",
    "        target_logit = cos_theta[torch.arange(0, embbedings.size(0)), label].view(-1, 1)\n",
    "\n",
    "        sin_theta = torch.sqrt(1.0 - torch.pow(target_logit, 2))\n",
    "        cos_theta_m = target_logit * self.cos_m - sin_theta * self.sin_m #cos(target+margin)\n",
    "        mask = cos_theta > cos_theta_m\n",
    "        final_target_logit = torch.where(target_logit > self.threshold, cos_theta_m, target_logit - self.mm)\n",
    "\n",
    "        hard_example = cos_theta[mask]\n",
    "        with torch.no_grad():\n",
    "            self.t = target_logit.mean() * 0.01 + (1 - 0.01) * self.t\n",
    "        cos_theta[mask] = hard_example * (self.t + hard_example)\n",
    "        cos_theta.scatter_(1, label.view(-1, 1).long(), final_target_logit)\n",
    "        output = cos_theta * self.s\n",
    "        return output #, nn.CrossEntropyLoss()(output,label)\n",
    "\n",
    "class MergeLossLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MergeLossLayer, self).__init__()\n",
    "        print('Using Merge Face')\n",
    "    \n",
    "    def forward(self, embbedings1, embbedings2, label):\n",
    "        embbedings = embbedings1 + embbedings2\n",
    "        scal = torch.tensor(np.ones((embbedings.shape[0],embbedings.shape[1])) * 2, device='cuda:1')\n",
    "        output = torch.div(embbedings, scal)\n",
    "        return output, nn.CrossEntropyLoss()(output,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-cleaner",
   "metadata": {
    "papermill": {
     "duration": 0.044831,
     "end_time": "2021-05-10T14:27:21.960027",
     "exception": false,
     "start_time": "2021-05-10T14:27:21.915196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imagenet-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "computational-korean",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.060282Z",
     "iopub.status.busy": "2021-05-10T14:27:22.059372Z",
     "iopub.status.idle": "2021-05-10T14:27:22.063471Z",
     "shell.execute_reply": "2021-05-10T14:27:22.062831Z"
    },
    "papermill": {
     "duration": 0.061224,
     "end_time": "2021-05-10T14:27:22.063609",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.002385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeModel_Two(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,model_name,\n",
    "        n_classes = CFG.classes,\n",
    "        fc_dim = 512,\n",
    "        margin = CFG.margin,\n",
    "        scale = CFG.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(ShopeeModel_Two,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if 'efficientnet' in model_name:\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        elif 'nfnet' in model_name:\n",
    "            final_in_features = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.0)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "        self.final = CurricularFace(final_in_features, \n",
    "                                           n_classes, \n",
    "                                           s=scale, \n",
    "                                           m=margin)\n",
    "\n",
    "        self.final2 = ArcMarginProduct(final_in_features,\n",
    "                                       n_classes,\n",
    "                                       scale = scale,\n",
    "                                       margin = margin,\n",
    "                                       easy_margin = False,\n",
    "                                       ls_eps = 0.0)\n",
    "    \n",
    "        self.Merge_final = MergeLossLayer()\n",
    "        \n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        #logits1 = self.final(feature,label) # Curr \n",
    "        #logits2 = self.final2(feature,label) # ArcFace\n",
    "        #logits = self.Merge_final(logits1, logits2, label)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-syntax",
   "metadata": {
    "papermill": {
     "duration": 0.04306,
     "end_time": "2021-05-10T14:27:22.149499",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.106439",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "concerned-robertson",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.245182Z",
     "iopub.status.busy": "2021-05-10T14:27:22.244252Z",
     "iopub.status.idle": "2021-05-10T14:27:22.248580Z",
     "shell.execute_reply": "2021-05-10T14:27:22.248049Z"
    },
    "papermill": {
     "duration": 0.055755,
     "end_time": "2021-05-10T14:27:22.248752",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.192997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeNet1(nn.Module): #DistilBert-Indonesian\n",
    "    def __init__(self, \n",
    "                 bert_model,\n",
    "                 num_classes=CFG2.NUM_CLASSES, \n",
    "                 last_hidden_size=CFG2.bert_hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert_model = bert_model\n",
    "\n",
    "    \n",
    "    def get_bert_features(self, batch):\n",
    "        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        last_hidden_state = output.last_hidden_state \n",
    "        CLS_token_state = last_hidden_state[0] \n",
    "        CLS_token_state = last_hidden_state[:, 0, :] \n",
    "        return CLS_token_state\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        CLS_hidden_state = self.get_bert_features(batch)\n",
    "        return F.normalize(CLS_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "korean-routine",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.344079Z",
     "iopub.status.busy": "2021-05-10T14:27:22.343076Z",
     "iopub.status.idle": "2021-05-10T14:27:22.347097Z",
     "shell.execute_reply": "2021-05-10T14:27:22.346450Z"
    },
    "papermill": {
     "duration": 0.054368,
     "end_time": "2021-05-10T14:27:22.347241",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.292873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ShopeeNet2(nn.Module): #Roberta\n",
    "    def __init__(self, \n",
    "                 roberta_model,\n",
    "                 num_classes=CFG2.NUM_CLASSES, \n",
    "                 last_hidden_size=CFG2.bert_hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.roberta_model = roberta_model\n",
    "\n",
    "    def get_bert_features(self, batch):\n",
    "        output = self.roberta_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "        last_hidden_state = output.last_hidden_state \n",
    "        CLS_token_state = last_hidden_state[0] \n",
    "        CLS_token_state = last_hidden_state[:, 0, :] \n",
    "        return CLS_token_state\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        CLS_hidden_state = self.get_bert_features(batch)\n",
    "        return F.normalize(CLS_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eligible-validity",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.449049Z",
     "iopub.status.busy": "2021-05-10T14:27:22.448053Z",
     "iopub.status.idle": "2021-05-10T14:27:22.451641Z",
     "shell.execute_reply": "2021-05-10T14:27:22.451041Z"
    },
    "papermill": {
     "duration": 0.059788,
     "end_time": "2021-05-10T14:27:22.451801",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.392013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Mish_func(torch.autograd.Function):\n",
    "    \n",
    "    \"\"\"from: https://github.com/tyunist/memory_efficient_mish_swish/blob/master/mish.py\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_variables[0]\n",
    "  \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2) \n",
    "\n",
    "        # Note that grad_hv * grad_vx = sigmoid(x)\n",
    "        #grad_hv = 1./v  \n",
    "        #grad_vx = i.exp()\n",
    "        \n",
    "        grad_hx = i.sigmoid()\n",
    "\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n",
    "        \n",
    "        return grad_output * grad_f \n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)\n",
    "\n",
    "\n",
    "def replace_activations(model, existing_layer, new_layer):\n",
    "    \n",
    "    \"\"\"A function for replacing existing activation layers\"\"\"\n",
    "    \n",
    "    for name, module in reversed(model._modules.items()):\n",
    "        if len(list(module.children())) > 0:\n",
    "            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n",
    "\n",
    "        if type(module) == existing_layer:\n",
    "            layer_old = module\n",
    "            layer_new = new_layer\n",
    "            model._modules[name] = layer_new\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-ozone",
   "metadata": {
    "papermill": {
     "duration": 0.044331,
     "end_time": "2021-05-10T14:27:22.539867",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.495536",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imagenet get_image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "broadband-medication",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.644305Z",
     "iopub.status.busy": "2021-05-10T14:27:22.636661Z",
     "iopub.status.idle": "2021-05-10T14:27:22.647383Z",
     "shell.execute_reply": "2021-05-10T14:27:22.647874Z"
    },
    "papermill": {
     "duration": 0.063877,
     "end_time": "2021-05-10T14:27:22.648062",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.584185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_predictions(df, embeddings,threshold = 0.0): # GET_CV=Ture일 경우 threshold값 지정과 상관 없음.(False일 경우 사용) \n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 50\n",
    "    else : \n",
    "        KNN = 3\n",
    "    print(f'KNN={KNN}')\n",
    "    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine')\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    \n",
    "    if GET_CV: # train 데이터로 CV score 계산 시작       \n",
    "        thresholds = list(np.arange(0.2, 3.0, 0.01)) # image prediction threshold 범위를 여기서 지정할 것 \n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            predictions = []\n",
    "            for k in range(embeddings.shape[0]):\n",
    "                idx = np.where(distances[k,] < threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "                predictions.append(posting_ids)\n",
    "            df['pred_matches'] = predictions\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}') # 여기서 best threshold 출력\n",
    "        \n",
    "        # best threshold로 최종 return 하도록 진행\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            idx = np.where(distances[k,] < best_threshold)[0] # best threshold가 들어감.\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "\n",
    "        del model, distances, indices\n",
    "        gc.collect()\n",
    "        return predictions # best threshold로 예측한 것을 return\n",
    "    \n",
    "    else: # test 데이터로 예측 - threshold는 수동으로 지정해야함.\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            # 사전에 train으로 best cv나온 threshold값을 함수 인자로서 수동으로 지정해줄 것!\n",
    "            idx = np.where(distances[k,] < threshold)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "\n",
    "        del model, distances, indices\n",
    "        gc.collect()\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-dividend",
   "metadata": {
    "papermill": {
     "duration": 0.043864,
     "end_time": "2021-05-10T14:27:22.735236",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.691372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "opened-firewall",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:22.838404Z",
     "iopub.status.busy": "2021-05-10T14:27:22.837366Z",
     "iopub.status.idle": "2021-05-10T14:27:22.840996Z",
     "shell.execute_reply": "2021-05-10T14:27:22.840411Z"
    },
    "papermill": {
     "duration": 0.061276,
     "end_time": "2021-05-10T14:27:22.841125",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.779849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths, model_name, model_path):\n",
    "    embeds = []\n",
    "    \n",
    "    if model_name == 'eca_nfnet_l0':\n",
    "        model = ShopeeModel_Two(model_name=model_name)\n",
    "        model.eval()\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "        model.load_state_dict(torch.load(model_path, map_location=\"cuda:0\"))\n",
    "        model = model.to(CFG.device)\n",
    "    \n",
    "    elif model_name == 'eca_nfnet_l1':\n",
    "        model = ShopeeModel_Two(model_name=model_name)\n",
    "        model.eval()\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "        model.load_state_dict(torch.load(model_path, map_location=\"cuda:0\"))\n",
    "        model = model.to(CFG.device)\n",
    "    \n",
    "    \n",
    "    elif model_name == 'efficientnet_b3':\n",
    "        model = ShopeeModel_Two(model_name=model_name)\n",
    "        model.eval()\n",
    "        model = replace_activations(model, torch.nn.SiLU, Mish())\n",
    "        model.load_state_dict(torch.load(model_path, map_location=\"cuda:0\"))\n",
    "        model = model.to(CFG.device)\n",
    "    \n",
    "    else:\n",
    "        model = ShopeeModel(model_name=model_name, LossTypes='ArcFace')\n",
    "        model.eval()\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model = model.to(CFG.device)\n",
    "\n",
    "    image_dataset = ShopeeDataset(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=CFG.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-appendix",
   "metadata": {
    "papermill": {
     "duration": 0.044402,
     "end_time": "2021-05-10T14:27:22.929281",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.884879",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TFIDF embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "unexpected-chrome",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.038677Z",
     "iopub.status.busy": "2021-05-10T14:27:23.031220Z",
     "iopub.status.idle": "2021-05-10T14:27:23.042191Z",
     "shell.execute_reply": "2021-05-10T14:27:23.041646Z"
    },
    "papermill": {
     "duration": 0.06886,
     "end_time": "2021-05-10T14:27:23.042325",
     "exception": false,
     "start_time": "2021-05-10T14:27:22.973465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_predictions(df, max_features=25_000):\n",
    "    \n",
    "    model = TfidfVectorizer(stop_words='english', binary=True, max_features=max_features) # stop_words='english'\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    print('Finding similar titles...')\n",
    "    CHUNK = 1024 * 4\n",
    "    CTS = len(df) // CHUNK\n",
    "    if (len(df)%CHUNK) != 0:\n",
    "        CTS += 1\n",
    "    thresholds = list(np.arange(0.55,0.8,0.025)) # text threshold 범위를 여기서 지정할것.\n",
    "    scores = []\n",
    "    if GET_CV:    \n",
    "        for threshold in thresholds:\n",
    "            preds = []\n",
    "            for j in range( CTS ):\n",
    "                a = j * CHUNK\n",
    "                b = (j+1) * CHUNK\n",
    "                b = min(b, len(df))\n",
    "                print('chunk', a, 'to', b)\n",
    "\n",
    "                # COSINE SIMILARITY DISTANCE\n",
    "                cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "                for k in range(b-a):\n",
    "                    IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                    o = ' '.join(o)\n",
    "                    preds.append(o)\n",
    "            df['pred_matches'] = preds\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        print(' ')\n",
    "        print(f'Final text embeddings prediction using best threshold {best_threshold}')\n",
    "        preds = []\n",
    "        for j in range( CTS ):\n",
    "            a = j * CHUNK\n",
    "            b = (j+1) * CHUNK\n",
    "            b = min(b, len(df))\n",
    "            print('chunk', a, 'to', b)\n",
    "            # COSINE SIMILARITY DISTANCE\n",
    "            cts = cupy.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>best_threshold)[0]\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "        del model,text_embeddings\n",
    "        gc.collect()\n",
    "        return preds # best cv score를 기록한 threshold값으로 return\n",
    "    \n",
    "    \n",
    "    else: # test 데이터로 예측 - threshold는 수동으로 지정해야함.\n",
    "        #model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n",
    "        #text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "        preds = []\n",
    "        CHUNK = 1024*4\n",
    "\n",
    "        print('Finding similar titles...')\n",
    "        CTS = len(df)//CHUNK\n",
    "        if len(df)%CHUNK!=0: CTS += 1\n",
    "        for j in range( CTS ):\n",
    "\n",
    "            a = j*CHUNK\n",
    "            b = (j+1)*CHUNK\n",
    "            b = min(b,len(df))\n",
    "            print('chunk',a,'to',b)\n",
    "\n",
    "            # COSINE SIMILARITY DISTANCE\n",
    "            cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
    "\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>0.75)[0] # 현재 defualt는 0.75. 여기서 train best cv score의 threshold를 수동으로 지정.\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "                \n",
    "        del model,text_embeddings\n",
    "        gc.collect()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-reserve",
   "metadata": {
    "papermill": {
     "duration": 0.043718,
     "end_time": "2021-05-10T14:27:23.130816",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.087098",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert One_epoch function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "combined-handy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.226740Z",
     "iopub.status.busy": "2021-05-10T14:27:23.225811Z",
     "iopub.status.idle": "2021-05-10T14:27:23.229540Z",
     "shell.execute_reply": "2021-05-10T14:27:23.228946Z"
    },
    "papermill": {
     "duration": 0.054661,
     "end_time": "2021-05-10T14:27:23.229710",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.175049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_epoch(model, \n",
    "              loader,\n",
    "              optimizer=None, \n",
    "              lr_scheduler=None):\n",
    "    \n",
    "    embeds = []\n",
    "    \n",
    "    tqdm_object = tqdm(loader, total=len(loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch = {k: v.to(CFG.device) for k, v in batch.items()}\n",
    "        feat = model(batch)\n",
    "        text_embeddings = feat.detach().cpu().numpy()\n",
    "        embeds.append(text_embeddings)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-norway",
   "metadata": {
    "papermill": {
     "duration": 0.044852,
     "end_time": "2021-05-10T14:27:23.319745",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.274893",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert cosine sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "responsible-comedy",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.428114Z",
     "iopub.status.busy": "2021-05-10T14:27:23.426960Z",
     "iopub.status.idle": "2021-05-10T14:27:23.430522Z",
     "shell.execute_reply": "2021-05-10T14:27:23.429889Z"
    },
    "papermill": {
     "duration": 0.067571,
     "end_time": "2021-05-10T14:27:23.430672",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.363101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_neighbours_cos_sim(df,embeddings):\n",
    "    '''\n",
    "    When using cos_sim use normalized features else use normal features\n",
    "    '''\n",
    "    embeddings = cupy.array(embeddings)\n",
    "    \n",
    "    if GET_CV:\n",
    "        thresholds = list(np.arange(0.5,0.7,0.05))\n",
    "\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            \n",
    "################################################# Code for Getting Preds #########################################\n",
    "            preds = []\n",
    "            CHUNK = 1024*4\n",
    "\n",
    "            print('Finding similar titles...for threshold :',threshold)\n",
    "            CTS = len(embeddings)//CHUNK\n",
    "            if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "            for j in range( CTS ):\n",
    "                a = j*CHUNK\n",
    "                b = (j+1)*CHUNK\n",
    "                b = min(b,len(embeddings))\n",
    "\n",
    "                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "                for k in range(b-a):\n",
    "                    IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                    o = ' '.join(o)\n",
    "                    preds.append(o)\n",
    "######################################################################################################################\n",
    "            df['pred_matches'] = preds\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "            \n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "            \n",
    "    else:\n",
    "        preds = []\n",
    "        CHUNK = 1024*4\n",
    "        threshold = 0.8\n",
    "\n",
    "        print('Finding similar texts...for threshold :',threshold)\n",
    "        CTS = len(embeddings)//CHUNK\n",
    "        if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "        for j in range( CTS ):\n",
    "            a = j*CHUNK\n",
    "            b = (j+1)*CHUNK\n",
    "            b = min(b,len(embeddings))\n",
    "            print('chunk',a,'to',b)\n",
    "\n",
    "            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "                    \n",
    "    return df, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "through-belief",
   "metadata": {
    "papermill": {
     "duration": 0.043253,
     "end_time": "2021-05-10T14:27:23.518547",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.475294",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "instant-canadian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.620645Z",
     "iopub.status.busy": "2021-05-10T14:27:23.619612Z",
     "iopub.status.idle": "2021-05-10T14:27:23.623372Z",
     "shell.execute_reply": "2021-05-10T14:27:23.622773Z"
    },
    "papermill": {
     "duration": 0.060645,
     "end_time": "2021-05-10T14:27:23.623511",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.562866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_text_embeddings_bert(df, model):\n",
    "    embeds = []\n",
    "    \n",
    "    if model == 'distil':\n",
    "        bert_model = DistilBertModel.from_pretrained('../input/distilbert-base-indonesian')\n",
    "        model = ShopeeNet1(bert_model)\n",
    "        model.load_state_dict(dict(list(torch.load(CFG2.TEXT_MODEL_PATH1).items())[:-1]))\n",
    "        model.eval()\n",
    "        model = model.to(CFG2.device)\n",
    "\n",
    "        model_name = '../input/distilbert-base-indonesian'\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "    elif model == 'roberta':\n",
    "        roberta_model = RobertaModel.from_pretrained('../input/roberta-base') \n",
    "        model = ShopeeNet2(roberta_model)\n",
    "        model.load_state_dict(dict(list(torch.load(CFG2.TEXT_MODEL_PATH2).items())[:-1]))\n",
    "        model.eval()\n",
    "        model = model.to(CFG2.device)\n",
    "\n",
    "        model_name = '../input/roberta-base'\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    elif model == 'distil_mergeface':\n",
    "        bert_model = DistilBertModel.from_pretrained('../input/distilbert-base-indonesian')\n",
    "        model = ShopeeNet1(bert_model)\n",
    "        model.load_state_dict(dict(list(torch.load(CFG2.TEXT_MODEL_PATH3).items())[:-3]))\n",
    "        model.eval()\n",
    "        model = model.to(CFG2.device)\n",
    "\n",
    "        model_name = '../input/distilbert-base-indonesian'\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    text_dataset = ShopeeDataset2(df, tokenizer, max_length=CFG2.max_length)  \n",
    "    text_loader = torch.utils.data.DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=CFG2.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=CFG2.num_workers\n",
    "    ) \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        embeds = one_epoch(model, \n",
    "                           text_loader, \n",
    "                           optimizer=None,\n",
    "                           lr_scheduler=None)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-behavior",
   "metadata": {
    "papermill": {
     "duration": 0.044486,
     "end_time": "2021-05-10T14:27:23.713013",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.668527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Bert Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "composite-rover",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.808561Z",
     "iopub.status.busy": "2021-05-10T14:27:23.807864Z",
     "iopub.status.idle": "2021-05-10T14:27:23.856688Z",
     "shell.execute_reply": "2021-05-10T14:27:23.856044Z"
    },
    "papermill": {
     "duration": 0.099723,
     "end_time": "2021-05-10T14:27:23.856872",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.757149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df,df_cu, image_paths = read_dataset()\n",
    "\n",
    "if GET_CV :\n",
    "    lbl_encoder = LabelEncoder()\n",
    "    df['label_code'] = lbl_encoder.fit_transform(df['label_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "activated-belize",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:23.954518Z",
     "iopub.status.busy": "2021-05-10T14:27:23.953849Z",
     "iopub.status.idle": "2021-05-10T14:27:35.639356Z",
     "shell.execute_reply": "2021-05-10T14:27:35.641405Z"
    },
    "papermill": {
     "duration": 11.740567,
     "end_time": "2021-05-10T14:27:35.642100",
     "exception": false,
     "start_time": "2021-05-10T14:27:23.901533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our text embeddings shape is (3, 768)\n",
      "CPU times: user 3.77 s, sys: 971 ms, total: 4.75 s\n",
      "Wall time: 11.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time finetuned_emb1 = get_text_embeddings_bert(df,'distil')\n",
    "# %time finetuned_emb2 = get_text_embeddings_bert(df,'roberta')\n",
    "# %time finetuned_emb3 = np.concatenate([finetuned_emb1,finetuned_emb2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "green-simple",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:35.852430Z",
     "iopub.status.busy": "2021-05-10T14:27:35.847931Z",
     "iopub.status.idle": "2021-05-10T14:27:37.171409Z",
     "shell.execute_reply": "2021-05-10T14:27:37.172713Z"
    },
    "papermill": {
     "duration": 1.422604,
     "end_time": "2021-05-10T14:27:37.173061",
     "exception": false,
     "start_time": "2021-05-10T14:27:35.750457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** DistillBert-Indonesian CV score *****\n",
      "Finding similar texts...for threshold : 0.8\n",
      "chunk 0 to 3\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"***** DistillBert-Indonesian CV score *****\")\n",
    "df_t1,t1 = get_neighbours_cos_sim(df,finetuned_emb1)\n",
    "print(\"-\"*80)\n",
    "# print(\"***** RoBerta CV score *****\")\n",
    "# df_t2,t2 = get_neighbours_cos_sim(df,finetuned_emb2)\n",
    "# print(\"-\"*80)\n",
    "\n",
    "# print(\"***** Concat Embedding CV score *****\")\n",
    "# df_t3,t3 = get_neighbours_cos_sim(df,finetuned_emb3)\n",
    "# print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-branch",
   "metadata": {
    "papermill": {
     "duration": 0.049044,
     "end_time": "2021-05-10T14:27:37.271401",
     "exception": false,
     "start_time": "2021-05-10T14:27:37.222357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## # Image&TFIDF Final inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "known-logan",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:37.382284Z",
     "iopub.status.busy": "2021-05-10T14:27:37.381161Z",
     "iopub.status.idle": "2021-05-10T14:27:48.238389Z",
     "shell.execute_reply": "2021-05-10T14:27:48.239028Z"
    },
    "papermill": {
     "duration": 10.919978,
     "end_time": "2021-05-10T14:27:48.239221",
     "exception": false,
     "start_time": "2021-05-10T14:27:37.319243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for eca_nfnet_l1 model\n",
      "Using Curricular Face\n",
      "Using ArcFace\n",
      "Using Merge Face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "Building Model Backbone for efficientnet_b3 model\n",
      "Using Curricular Face\n",
      "Using ArcFace\n",
      "Using Merge Face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "Building Model Backbone for eca_nfnet_l0 model\n",
      "Using Curricular Face\n",
      "Using ArcFace\n",
      "Using Merge Face\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our image embeddings shape is (3, 512)\n",
      "final embeddings ensemble shape : (3, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_embeddings1 = get_image_embeddings(image_paths.values, CFG.model_name1, CFG.model_path1) # NFl0(m)\n",
    "image_embeddings2 = get_image_embeddings(image_paths.values, CFG.model_name2, CFG.model_path2) # B3(merge)\n",
    "image_embeddings3 = get_image_embeddings(image_paths.values, CFG.model_name3, CFG.model_path3) # NFl1(m)\n",
    "\n",
    "image_embeddings = np.concatenate([image_embeddings1,image_embeddings2, image_embeddings3],axis=1)\n",
    "\n",
    "# 최종 image embeddings shape를 확인\n",
    "print(f'final embeddings ensemble shape : {image_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "inside-sculpture",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:48.357500Z",
     "iopub.status.busy": "2021-05-10T14:27:48.356423Z",
     "iopub.status.idle": "2021-05-10T14:27:49.002842Z",
     "shell.execute_reply": "2021-05-10T14:27:49.001915Z"
    },
    "papermill": {
     "duration": 0.707516,
     "end_time": "2021-05-10T14:27:49.003068",
     "exception": false,
     "start_time": "2021-05-10T14:27:48.295552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 2726.52it/s]\n"
     ]
    }
   ],
   "source": [
    "image_predictions = get_image_predictions(df, image_embeddings, threshold = 0.36)#0.36 # train으로 먼저 best threshold를 찾을 것!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlike-fountain",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:27:49.123165Z",
     "iopub.status.busy": "2021-05-10T14:27:49.122342Z",
     "iopub.status.idle": "2021-05-10T14:28:06.013828Z",
     "shell.execute_reply": "2021-05-10T14:28:06.014338Z"
    },
    "papermill": {
     "duration": 16.952636,
     "end_time": "2021-05-10T14:28:06.014524",
     "exception": false,
     "start_time": "2021-05-10T14:27:49.061888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "Finding similar titles...\n",
      "chunk 0 to 3\n"
     ]
    }
   ],
   "source": [
    "text_predictions = get_text_predictions(df, max_features = 21_500) # tfidf_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprising-physiology",
   "metadata": {
    "papermill": {
     "duration": 0.054935,
     "end_time": "2021-05-10T14:28:06.126853",
     "exception": false,
     "start_time": "2021-05-10T14:28:06.071918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Combine function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abstract-syracuse",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:28:06.246181Z",
     "iopub.status.busy": "2021-05-10T14:28:06.245347Z",
     "iopub.status.idle": "2021-05-10T14:28:06.250502Z",
     "shell.execute_reply": "2021-05-10T14:28:06.249651Z"
    },
    "papermill": {
     "duration": 0.068211,
     "end_time": "2021-05-10T14:28:06.250662",
     "exception": false,
     "start_time": "2021-05-10T14:28:06.182451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hash 제외할 시 \n",
    "def combine_predictions(row):\n",
    "    x = np.concatenate([row['image_predictions'], row['text_predictions'],row['distilbert_predictions']]) \n",
    "    return ' '.join( np.unique(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "prompt-uncle",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:28:06.377277Z",
     "iopub.status.busy": "2021-05-10T14:28:06.374803Z",
     "iopub.status.idle": "2021-05-10T14:28:06.378019Z",
     "shell.execute_reply": "2021-05-10T14:28:06.378582Z"
    },
    "papermill": {
     "duration": 0.071362,
     "end_time": "2021-05-10T14:28:06.378772",
     "exception": false,
     "start_time": "2021-05-10T14:28:06.307410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "def intersect(*args):\n",
    "    return reduce(np.intersect1d, args)\n",
    "\n",
    "def concat(*args):\n",
    "    return np.unique(np.concatenate(args))\n",
    "\n",
    "def higher(f,*args):\n",
    "    res = {}\n",
    "    keys = np.unique(np.concatenate(args)) #중복없이 제출 합집합 생성\n",
    "    for k in keys: \n",
    "        res[k] = np.count_nonzero(np.concatenate(args) == k)\n",
    "    output_dict = dict(filter(lambda item: item[1] >= f, res.items()))\n",
    "    \n",
    "    return np.array(list(output_dict.keys()))\n",
    "\n",
    "def count(*args):\n",
    "    res = {}\n",
    "    keys = np.unique(np.concatenate(args))\n",
    "    for k in keys: \n",
    "        res[k] = np.count_nonzero(np.concatenate(args) == k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-arrival",
   "metadata": {
    "papermill": {
     "duration": 0.057282,
     "end_time": "2021-05-10T14:28:06.493477",
     "exception": false,
     "start_time": "2021-05-10T14:28:06.436195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stock-communist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-10T14:28:06.618850Z",
     "iopub.status.busy": "2021-05-10T14:28:06.617802Z",
     "iopub.status.idle": "2021-05-10T14:28:06.792783Z",
     "shell.execute_reply": "2021-05-10T14:28:06.792128Z"
    },
    "papermill": {
     "duration": 0.242855,
     "end_time": "2021-05-10T14:28:06.792946",
     "exception": false,
     "start_time": "2021-05-10T14:28:06.550091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Concatenate image predctions with text predictions\n",
    "# tmp = df.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "# df['oof_hash'] = df.image_phash.map(tmp)\n",
    "\n",
    "if GET_CV:\n",
    "    df['image_predictions'] = image_predictions\n",
    "    #df['text_word2vec'] = non_dup_train_df['test_matches']\n",
    "    df['text_predictions_bert'] = text_predictions_bert\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['pred_matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "    score = df['f1'].mean()\n",
    "    print(f'Our final f1 cv score is {score}')\n",
    "    df['matches'] = df['pred_matches']\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n",
    "else:\n",
    "    df['image_predictions'] = image_predictions\n",
    "#     df['text_predictions_bert'] = text_predictions_bert\n",
    "    df['text_predictions'] = text_predictions\n",
    "    df['distilbert_predictions'] = t1\n",
    "#     df['roberta_predictions'] = t2\n",
    "# #     df['concat_predictions'] = t3\n",
    "#     p1 = [x for x in df['image_predictions']]\n",
    "#     p2 = [x for x in df['text_predictions']]\n",
    "#     p3 = [x for x in df['distilbert_predictions']]\n",
    "#     df['matches'] = higher(2,p1,p2,p3)\n",
    "    df['matches'] = df.apply(combine_predictions, axis = 1)\n",
    "    df[['posting_id', 'matches']].to_csv('submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 76.846698,
   "end_time": "2021-05-10T14:28:09.933634",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-10T14:26:53.086936",
   "version": "2.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}