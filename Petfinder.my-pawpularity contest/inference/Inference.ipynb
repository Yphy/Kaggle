{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sitting-steal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Asthetics\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "# General\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import cv2\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Image Aug\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything()\n",
    "\n",
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adapted-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file : /mnt/hdd1/wearly/kaggle/petfinder/data/test.csv\n",
      "Models path : /mnt/hdd1/wearly/kaggle/petfinder/data/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "csv_dir = '/mnt/hdd1/wearly/kaggle/petfinder/data/'\n",
    "test_dir = '/mnt/hdd1/wearly/kaggle/petfinder/data/test/'\n",
    "models_dir = '/mnt/hdd1/wearly/ethan/petfinder/vit_large_patch32_384'\n",
    "\n",
    "test_file_path = '/mnt/hdd1/wearly/kaggle/petfinder/data/test.csv'\n",
    "sample_sub_file_path = os.path.join(csv_dir, 'sample_submission.csv')\n",
    "\n",
    "print(f'Test file : {test_file_path}')\n",
    "print(f'Models path : {sample_sub_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pregnant-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_file_path)\n",
    "sample_df = pd.read_csv(sample_sub_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "willing-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_filpath(name, folder):\n",
    "    path = os.path.join(folder, f'{name}.jpg')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "general-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['image_path'] = test_df['Id'].apply(lambda x: return_filpath(x, test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-burden",
   "metadata": {},
   "source": [
    "## CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "artificial-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model':'vit_large_patch32_384',\n",
    "    'dense_features': ['Subject Focus', 'Eyes', 'Face', 'Near',\n",
    "                       'Action', 'Accessory', 'Group', 'Collage',\n",
    "                       'Human', 'Occlusion', 'Info', 'Blur'],\n",
    "    'pretrained': False,\n",
    "    'inp_channels': 3,\n",
    "    'im_size': 384,\n",
    "    'device': device,\n",
    "    'batch_size': 16,\n",
    "    'num_workers' : 2,\n",
    "    'out_features': 1,\n",
    "    'debug': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "built-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "if params['debug']:\n",
    "    test_df = test_df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-choir",
   "metadata": {},
   "source": [
    "## Aubgmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "breathing-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms(DIM=params['im_size']):\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(DIM,DIM),\n",
    "        albumentations.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-mother",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjustable-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuteDataset(Dataset):\n",
    "    def __init__(self, images_filepaths, dense_features, targets, transform=None):\n",
    "        self.images_filepaths = images_filepaths\n",
    "        self.dense_features = dense_features\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.images_filepaths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=image)['image']\n",
    "        \n",
    "        dense = self.dense_features[idx, :]\n",
    "        label = torch.tensor(self.targets[idx]).float()\n",
    "        return image, dense, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-rates",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instrumental-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetNet(nn.Module):\n",
    "    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],\n",
    "                 pretrained=params['pretrained'], num_dense=len(params['dense_features'])):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)\n",
    "        n_features = self.model.head.in_features\n",
    "        self.model.head = nn.Linear(n_features, 128)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 + num_dense, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, out_features)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, image, dense):\n",
    "        embeddings = self.model(image)\n",
    "        x = self.dropout(embeddings)\n",
    "        x = torch.cat([x, dense], dim=1)\n",
    "        output = self.fc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-plate",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "banner-canyon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3716a511488941f18b807c910f2c6292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting. ', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc8ae1e12634f9a86731ee18f3fd95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting. ', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb25b4f11ba438fa1746620c8d4dcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting. ', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fe8a03a24f400d976e5b3654e8b695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting. ', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ece9cb85c554fb794a0b36235996ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Predicting. ', max=1.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = None\n",
    "for model_name in glob.glob(models_dir + '/*.pth'):\n",
    "    model = PetNet()\n",
    "    model.load_state_dict(torch.load(model_name))\n",
    "    model = model.to(params['device'])\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = CuteDataset(\n",
    "        images_filepaths = test_df['image_path'].values,\n",
    "        dense_features = test_df[params['dense_features']].values,\n",
    "        targets = sample_df['Pawpularity'].values,\n",
    "        transform = get_test_transforms()\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=params['batch_size'],\n",
    "        shuffle=False, num_workers=params['num_workers'],\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    \n",
    "    temp_preds = None\n",
    "    with torch.no_grad():\n",
    "        for (images, dense, target) in tqdm(test_loader, desc=f'Predicting. '):\n",
    "            images = images.to(params['device'], non_blocking=True)\n",
    "            dense = dense.to(params['device'], non_blocking=True)\n",
    "            predictions = torch.sigmoid(model(images, dense)).to('cpu').numpy()*100\n",
    "            \n",
    "            if temp_preds is None:\n",
    "                temp_preds = predictions\n",
    "            else:\n",
    "                temp_preds = np.vstack((temp_preds, predictions))\n",
    "\n",
    "    if predicted_labels is None:\n",
    "        predicted_labels = temp_preds\n",
    "    else:\n",
    "        predicted_labels += temp_preds\n",
    "        \n",
    "predicted_labels /= (len(glob.glob(models_dir + '/*.pth')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
