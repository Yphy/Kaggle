{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southern-pastor",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simple-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "\n",
    "# Utils\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "import timm\n",
    "\n",
    "# Albumentations for augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# For colored terminal text\n",
    "from colorama import Fore, Back, Style\n",
    "c_ = Fore.CYAN\n",
    "sr_ = Style.RESET_ALL\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# For descriptive error messages\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "broken-converter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \n",
      "Get your W&B access token from here: https://wandb.ai/authorize\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    api_key = user_secrets.get_secret(\"b4d6bb52d1e0804563e845703788199f6ed67350\")\n",
    "    wandb.login(key=api_key)\n",
    "    anony = None\n",
    "except:\n",
    "    anony = \"must\"\n",
    "    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sound-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    seed = 42,\n",
    "    backbone = 'swin_base_patch4_window7_224',\n",
    "    embedder = 'tf_efficientnet_b4_ns',\n",
    "    train_batch_size = 16,\n",
    "    valid_batch_size = 32,\n",
    "    img_size = 448,\n",
    "    epochs = 5,\n",
    "    learning_rate = 1e-4,\n",
    "    scheduler = 'CosineAnnealingLR',\n",
    "    min_lr = 1e-6,\n",
    "    T_max = 100,\n",
    "#     T_0 = 25,\n",
    "#     warmup_epochs = 0,\n",
    "    weight_decay = 1e-6,\n",
    "    n_accumulate = 1,\n",
    "    n_fold = 5,\n",
    "    num_classes = 1,\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    competition = 'PetFinder',\n",
    "    _wandb_kernel = 'deb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "perfect-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "set_seed(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "atomic-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_file_path(id):\n",
    "    return f\"{TRAIN_DIR}/{id}.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opened-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '/mnt/hdd1/wearly/kaggle/petfinder/data/'\n",
    "TRAIN_DIR  = '/mnt/hdd1/wearly/kaggle/petfinder/data/train'\n",
    "df = pd.read_csv(f\"{ROOT_DIR}/train.csv\")\n",
    "df['file_path'] = df['Id'].apply(get_train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "certified-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col not in ['Id', 'Pawpularity', 'file_path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "formal-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(df, n_s=5, n_grp = None):\n",
    "    df['kfold'] = -1\n",
    "    \n",
    "    if n_grp is None:\n",
    "        skf = KFold(n_splits=n_s, random_state = CONFIG['seed'])\n",
    "        target = df['Pawpularity']\n",
    "    else:\n",
    "        skf = StratifiedKFold(n_splits=n_s, shuffle = True, random_state = CONFIG['seed'])\n",
    "        df['grp'] = pd.cut(df['Pawpularity'], n_grp, labels=False) #Pawpularity를 지정 범주로 길이 나누기\n",
    "        target = df.grp\n",
    "        \n",
    "    for fold_no, (t, v) in enumerate(skf.split(target,target)):\n",
    "        df.loc[v, 'kfold'] = fold_no\n",
    "        \n",
    "    df = df.drop('grp',axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "catholic-antigua",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>file_path</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>/mnt/hdd1/wearly/kaggle/petfinder/data/train/0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0009c66b9439883ba2750fb825e1d7db</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>/mnt/hdd1/wearly/kaggle/petfinder/data/train/0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>/mnt/hdd1/wearly/kaggle/petfinder/data/train/0...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>/mnt/hdd1/wearly/kaggle/petfinder/data/train/0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001dc955e10590d3ca4673f034feeef2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>/mnt/hdd1/wearly/kaggle/petfinder/data/train/0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Subject Focus  Eyes  Face  Near  Action  \\\n",
       "0  0007de18844b0dbbb5e1f607da0606e0              0     1     1     1       0   \n",
       "1  0009c66b9439883ba2750fb825e1d7db              0     1     1     0       0   \n",
       "2  0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1       0   \n",
       "3  0018df346ac9c1d8413cfcc888ca8246              0     1     1     1       0   \n",
       "4  001dc955e10590d3ca4673f034feeef2              0     0     0     1       0   \n",
       "\n",
       "   Accessory  Group  Collage  Human  Occlusion  Info  Blur  Pawpularity  \\\n",
       "0          0      1        0      0          0     0     0           63   \n",
       "1          0      0        0      0          0     0     0           42   \n",
       "2          0      0        0      1          1     0     0           28   \n",
       "3          0      0        0      0          0     0     0           15   \n",
       "4          0      1        0      0          0     0     0           72   \n",
       "\n",
       "                                           file_path  kfold  \n",
       "0  /mnt/hdd1/wearly/kaggle/petfinder/data/train/0...      0  \n",
       "1  /mnt/hdd1/wearly/kaggle/petfinder/data/train/0...      2  \n",
       "2  /mnt/hdd1/wearly/kaggle/petfinder/data/train/0...      0  \n",
       "3  /mnt/hdd1/wearly/kaggle/petfinder/data/train/0...      3  \n",
       "4  /mnt/hdd1/wearly/kaggle/petfinder/data/train/0...      4  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = create_folds(df, n_s=CONFIG['n_fold'], n_grp=14) #n_grp 의미?\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-maldives",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "several-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PawpularityDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.file_names = df['file_path'].values\n",
    "        self.targets = df['Pawpularity'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.file_names[index]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        target = self.targets[index]\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-chamber",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "welsh-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "        A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], \n",
    "                std=[0.229, 0.224, 0.225], \n",
    "                max_pixel_value=255.0, \n",
    "                p=1.0\n",
    "            ),\n",
    "        ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mental-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridEmbed(nn.Module):\n",
    "    \"\"\" CNN Feature Map Embedding\n",
    "    Extract feature map from CNN, flatten, project to embedding dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, img_size=224, patch_size=1, feature_size=None, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        assert isinstance(backbone, nn.Module)\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.backbone = backbone\n",
    "        if feature_size is None:\n",
    "            with torch.no_grad():\n",
    "                # NOTE Most reliable way of determining output dims is to run forward pass\n",
    "                training = backbone.training\n",
    "                if training:\n",
    "                    backbone.eval()\n",
    "                o = self.backbone(torch.zeros(1, in_chans, img_size[0], img_size[1]))\n",
    "                if isinstance(o, (list, tuple)):\n",
    "                    o = o[-1]  # last feature if backbone outputs list/tuple of features\n",
    "                feature_size = o.shape[-2:]\n",
    "                feature_dim = o.shape[1]\n",
    "                backbone.train(training)\n",
    "        else:\n",
    "            feature_size = (feature_size, feature_size)\n",
    "            if hasattr(self.backbone, 'feature_info'):\n",
    "                feature_dim = self.backbone.feature_info.channels()[-1]\n",
    "            else:\n",
    "                feature_dim = self.backbone.num_features\n",
    "        assert feature_size[0] % patch_size[0] == 0 and feature_size[1] % patch_size[1] == 0\n",
    "        self.grid_size = (feature_size[0] // patch_size[0], feature_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "        self.proj = nn.Conv2d(feature_dim, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            x = x[-1]  # last feature if backbone outputs list/tuple of features\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "injured-puppy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PawpularityModel(nn.Module):\n",
    "    def __init__(self, backbone, embedder, pretrained=True):\n",
    "        super(PawpularityModel, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=pretrained)\n",
    "2        self.backbone.patch_embed = HybridEmbed(self.embedder, img_size=CONFIG['img_size'], embed_dim=128)\n",
    "        self.n_features = self.backbone.head.in_features\n",
    "        self.backbone.reset_classifier(0)\n",
    "        self.fc = nn.Linear(self.n_features, CONFIG['num_classes'])\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.backbone(images)              # features = (bs, embedding_size)\n",
    "        output = self.fc(features)                    # outputs  = (bs, num_classes)\n",
    "        return output\n",
    "    \n",
    "model = PawpularityModel(CONFIG['backbone'], CONFIG['embedder'])\n",
    "model.to(CONFIG['device']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "classical-basketball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0644]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "img = torch.randn(1, 3, CONFIG['img_size'], CONFIG['img_size']).to(CONFIG['device'])\n",
    "model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-pollution",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relevant-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(outputs, targets):\n",
    "    return torch.sqrt(nn.MSELoss()(outputs.view(-1), targets.view(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personal-question",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flexible-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "    scaler = amp.GradScaler()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, (images, targets) in bar:         \n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        with amp.autocast(enabled=True):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "                \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-scenario",
   "metadata": {},
   "source": [
    "## Validation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hazardous-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    TARGETS = []\n",
    "    PREDS = []\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, (images, targets) in bar:        \n",
    "        images = images.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.float)\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        PREDS.append(outputs.view(-1).cpu().detach().numpy())\n",
    "        TARGETS.append(targets.view(-1).cpu().detach().numpy())\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])   \n",
    "    \n",
    "    TARGETS = np.concatenate(TARGETS)\n",
    "    PREDS = np.concatenate(PREDS)\n",
    "    val_rmse = mean_squared_error(TARGETS, PREDS, squared=False)\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, val_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-moment",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "legislative-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "    # To automatically log gradients\n",
    "    wandb.watch(model, log_freq=100)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_rmse = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, \n",
    "                                           dataloader=train_loader, \n",
    "                                           device=CONFIG['device'], epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss, val_epoch_rmse = valid_one_epoch(model, valid_loader, \n",
    "                                                         device=CONFIG['device'], \n",
    "                                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        history['Valid RMSE'].append(val_epoch_rmse)\n",
    "        \n",
    "        # Log the metrics\n",
    "        wandb.log({\"Train Loss\": train_epoch_loss})\n",
    "        wandb.log({\"Valid Loss\": val_epoch_loss})\n",
    "        wandb.log({\"Valid RMSE\": val_epoch_rmse})\n",
    "        \n",
    "        print(f'Valid RMSE: {val_epoch_rmse}')\n",
    "        \n",
    "        # deep copy the model\n",
    "        if val_epoch_rmse <= best_epoch_rmse:\n",
    "            print(f\"{c_}Validation Loss Improved ({best_epoch_rmse} ---> {val_epoch_rmse})\")\n",
    "            best_epoch_rmse = val_epoch_rmse\n",
    "            run.summary[\"Best RMSE\"] = best_epoch_rmse\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = \"RMSE{:.4f}_epoch{:.0f}.bin\".format(best_epoch_rmse, epoch)\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            wandb.save(PATH)\n",
    "            print(f\"Model Saved{sr_}\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best RMSE: {:.4f}\".format(best_epoch_rmse))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "earlier-liechtenstein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders(fold):\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = PawpularityDataset(TRAIN_DIR, df_train, transforms=data_transforms['train'])\n",
    "    valid_dataset = PawpularityDataset(TRAIN_DIR, df_valid, transforms=data_transforms['valid'])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n",
    "                              num_workers=4, shuffle=True, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n",
    "                              num_workers=4, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "legal-state",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_scheduler(optimizer):\n",
    "    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n",
    "                                                             eta_min=CONFIG['min_lr'])\n",
    "    elif CONFIG['scheduler'] == None:\n",
    "        return None\n",
    "        \n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-mention",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "historical-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = prepare_loaders(fold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-ferry",
   "metadata": {},
   "source": [
    "### Define Optimizer and Scheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "advisory-survey",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = fetch_scheduler(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "perceived-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myphy\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/yphy/Pawpularity/runs/fabnahtw\" target=\"_blank\">easy-jazz-3</a></strong> to <a href=\"https://wandb.ai/yphy/Pawpularity\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Pawpularity', \n",
    "                 config=CONFIG,\n",
    "                 entity=\"wearly\"\n",
    "                 job_type='Train',\n",
    "                 anonymous='must')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-omaha",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ambient-location",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using GPU: NVIDIA GeForce RTX 3090\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [03:46<00:00,  2.18it/s, Epoch=1, LR=1.61e-6, Train_Loss=20.5]\n",
      "100%|██████████| 62/62 [00:17<00:00,  3.52it/s, Epoch=1, LR=1.61e-6, Valid_Loss=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSE: 20.62921905517578\n",
      "\u001b[36mValidation Loss Improved (inf ---> 20.62921905517578)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [03:48<00:00,  2.17it/s, Epoch=2, LR=9.76e-5, Train_Loss=20]  \n",
      "100%|██████████| 62/62 [00:18<00:00,  3.44it/s, Epoch=2, LR=9.76e-5, Valid_Loss=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSE: 20.588619232177734\n",
      "\u001b[36mValidation Loss Improved (20.62921905517578 ---> 20.588619232177734)\n",
      "Model Saved\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [03:47<00:00,  2.18it/s, Epoch=3, LR=6.4e-6, Train_Loss=20.1] \n",
      "100%|██████████| 62/62 [00:16<00:00,  3.67it/s, Epoch=3, LR=6.4e-6, Valid_Loss=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSE: 20.598278045654297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [03:46<00:00,  2.18it/s, Epoch=4, LR=9.05e-5, Train_Loss=20.1]\n",
      "100%|██████████| 62/62 [00:16<00:00,  3.67it/s, Epoch=4, LR=9.05e-5, Valid_Loss=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSE: 20.613536834716797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 495/495 [03:46<00:00,  2.19it/s, Epoch=5, LR=1.55e-5, Train_Loss=20]  \n",
      "100%|██████████| 62/62 [00:16<00:00,  3.68it/s, Epoch=5, LR=1.55e-5, Valid_Loss=20.4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid RMSE: 20.589576721191406\n",
      "\n",
      "Training complete in 0h 20m 25s\n",
      "Best RMSE: 20.5886\n"
     ]
    }
   ],
   "source": [
    "model, history = run_training(model, optimizer, scheduler,\n",
    "                              device=CONFIG['device'],\n",
    "                              num_epochs=CONFIG['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "designing-finance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 91658... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 794.55MB of 794.55MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>█▁▂▁▁</td></tr><tr><td>Valid Loss</td><td>█▁▂▅▁</td></tr><tr><td>Valid RMSE</td><td>█▁▃▅▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Best RMSE</td><td>20.58862</td></tr><tr><td>Train Loss</td><td>20.04649</td></tr><tr><td>Valid Loss</td><td>20.35331</td></tr><tr><td>Valid RMSE</td><td>20.58958</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">easy-jazz-3</strong>: <a href=\"https://wandb.ai/yphy/Pawpularity/runs/fabnahtw\" target=\"_blank\">https://wandb.ai/yphy/Pawpularity/runs/fabnahtw</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211110_024724-fabnahtw/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-batch",
   "metadata": {},
   "source": [
    "## View Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cheap-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"720\"\n",
       "            src=\"https://wandb.ai/yphy/Pawpularity/runs/fabnahtw\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f65dd0cbd68>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code taken from https://www.kaggle.com/ayuraj/interactive-eda-using-w-b-tables\n",
    "\n",
    "# This is just to display the W&B run page in this interactive session.\n",
    "from IPython import display\n",
    "\n",
    "# we create an IFrame and set the width and height\n",
    "iF = display.IFrame(run.url, width=1000, height=720)\n",
    "iF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
