{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "northern-perry",
   "metadata": {},
   "source": [
    "* https://www.kaggle.com/tanulsingh077/reaching-0-612-with-text-only-shopee?scriptVersionId=59862390"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-proxy",
   "metadata": {},
   "source": [
    "위 SBERT의 Inference코드를 참조하여 짜보려고한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "radio-facility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import (BertTokenizer, BertModel,\n",
    "                          DistilBertTokenizer, DistilBertModel)\n",
    "\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import cudf\n",
    "import cuml\n",
    "import cupy\n",
    "from cuml import PCA\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import Normalizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noble-priority",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/mnt/hdd1/wearly/kaggle/shopee/'\n",
    "train= pd.read_csv(data_path+'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surprising-flesh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this submission notebook will compute CV score, but commit notebook will not\n"
     ]
    }
   ],
   "source": [
    "NUM_WORKERS = 4\n",
    "BATCH_SIZE = 32\n",
    "SEED = 42\n",
    "DistilBERT = True\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "################################################  ADJUSTING FOR CV OR SUBMIT ##############################################\n",
    "\n",
    "CHECK_SUB = False\n",
    "GET_CV = True\n",
    "\n",
    "test = pd.read_csv(data_path+'test.csv')\n",
    "if len(test)>3: GET_CV = False\n",
    "else: print('this submission notebook will compute CV score, but commit notebook will not')\n",
    "\n",
    "\n",
    "################################################# MODEL ####################################################################\n",
    "\n",
    "model_name='cahya/distilbert-base-indonesian'\n",
    "TOKENIZER = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "################################################ MODEL PATH ###############################################################\n",
    "\n",
    "TEXT_MODEL_PATH = '/mnt/hdd1/wearly/ethan/shopee/Train/weights/model_100epochs.pt'\n",
    "\n",
    "model_params = {\n",
    "    'n_classes':11014,\n",
    "    'model_name':model_name,\n",
    "    'use_fc':False,\n",
    "    'dropout':0.3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-authority",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "informed-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset():\n",
    "    if GET_CV:\n",
    "        df = pd.read_csv(data_path+'train.csv')\n",
    "        tmp = df.groupby(['label_group'])['posting_id'].unique().to_dict()\n",
    "        df['matches'] = df['label_group'].map(tmp)\n",
    "        df['matches'] = df['matches'].apply(lambda x: ' '.join(x))\n",
    "        if CHECK_SUB:\n",
    "            df = pd.concat([df, df], axis = 0)\n",
    "            df.reset_index(drop = True, inplace = True)\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "    else:\n",
    "        df = pd.read_csv(data_path+'test.csv')\n",
    "        df_cu = cudf.DataFrame(df)\n",
    "        \n",
    "    return df, df_cu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-grenada",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "major-myrtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unable-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    len_y_pred = y_pred.apply(lambda x: len(x)).values\n",
    "    len_y_true = y_true.apply(lambda x: len(x)).values\n",
    "    f1 = 2 * intersection / (len_y_pred + len_y_true)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "reasonable-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors_knn(df, embeddings, KNN = 50):\n",
    "    '''\n",
    "    https://www.kaggle.com/ragnar123/unsupervised-baseline-arcface?scriptVersionId=57121538\n",
    "    '''\n",
    "\n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    model.fit(embeddings)\n",
    "    distances, indices = model.kneighbors(embeddings)\n",
    "    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n",
    "    if GET_CV:\n",
    "        thresholds = list(np.arange(0.6,0.8,0.05))\n",
    "        \n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            predictions = []\n",
    "            for k in range(embeddings.shape[0]):\n",
    "                idx = np.where(distances[k,] < threshold)[0]\n",
    "                ids = indices[k,idx]\n",
    "                posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "                predictions.append(posting_ids)\n",
    "            df['pred_matches'] = predictions\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "        \n",
    "        # Use threshold\n",
    "        predictions = []\n",
    "        for k in range(embeddings.shape[0]):\n",
    "            # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "            idx = np.where(distances[k,] < 0.60)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n",
    "            predictions.append(posting_ids)\n",
    "    \n",
    "    # Because we are predicting the test set that have 70K images and different label groups, confidence should be smaller\n",
    "    else:\n",
    "        predictions = []\n",
    "        for k in tqdm(range(embeddings.shape[0])):\n",
    "            idx = np.where(distances[k,] < 0.60)[0]\n",
    "            ids = indices[k,idx]\n",
    "            posting_ids = df['posting_id'].iloc[ids].values\n",
    "            predictions.append(posting_ids)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    gc.collect()\n",
    "    return df, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "noted-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbours_cos_sim(df,embeddings):\n",
    "    '''\n",
    "    When using cos_sim use normalized features else use normal features\n",
    "    '''\n",
    "    embeddings = cupy.array(embeddings)\n",
    "    \n",
    "    if GET_CV:\n",
    "        thresholds = list(np.arange(0.5,0.7,0.05))\n",
    "\n",
    "        scores = []\n",
    "        for threshold in thresholds:\n",
    "            \n",
    "################################################# Code for Getting Preds #########################################\n",
    "            preds = []\n",
    "            CHUNK = 1024*4\n",
    "\n",
    "            print('Finding similar titles...for threshold :',threshold)\n",
    "            CTS = len(embeddings)//CHUNK\n",
    "            if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "            for j in range( CTS ):\n",
    "                a = j*CHUNK\n",
    "                b = (j+1)*CHUNK\n",
    "                b = min(b,len(embeddings))\n",
    "\n",
    "                cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "                for k in range(b-a):\n",
    "                    IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                    o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                    o = ' '.join(o)\n",
    "                    preds.append(o)\n",
    "######################################################################################################################\n",
    "            df['pred_matches'] = preds\n",
    "            df['f1'] = f1_score(df['matches'], df['pred_matches'])\n",
    "            score = df['f1'].mean()\n",
    "            print(f'Our f1 score for threshold {threshold} is {score}')\n",
    "            scores.append(score)\n",
    "            \n",
    "        thresholds_scores = pd.DataFrame({'thresholds': thresholds, 'scores': scores})\n",
    "        max_score = thresholds_scores[thresholds_scores['scores'] == thresholds_scores['scores'].max()]\n",
    "        best_threshold = max_score['thresholds'].values[0]\n",
    "        best_score = max_score['scores'].values[0]\n",
    "        print(f'Our best score is {best_score} and has a threshold {best_threshold}')\n",
    "            \n",
    "    else:\n",
    "        preds = []\n",
    "        CHUNK = 1024*4\n",
    "        threshold = 0.7\n",
    "\n",
    "        print('Finding similar texts...for threshold :',threshold)\n",
    "        CTS = len(embeddings)//CHUNK\n",
    "        if len(embeddings)%CHUNK!=0: CTS += 1\n",
    "\n",
    "        for j in range( CTS ):\n",
    "            a = j*CHUNK\n",
    "            b = (j+1)*CHUNK\n",
    "            b = min(b,len(embeddings))\n",
    "            print('chunk',a,'to',b)\n",
    "\n",
    "            cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n",
    "\n",
    "            for k in range(b-a):\n",
    "                IDX = cupy.where(cts[k,]>threshold)[0]\n",
    "                o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "                preds.append(o)\n",
    "                    \n",
    "    return df, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-formation",
   "metadata": {},
   "source": [
    "## Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "promotional-advantage",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.csv.iloc[index]\n",
    "        \n",
    "        text = row.title\n",
    "        \n",
    "        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=30, return_tensors=\"pt\")\n",
    "        input_ids = text['input_ids'][0]\n",
    "        attention_mask = text['attention_mask'][0]  \n",
    "        \n",
    "        return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "considered-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "transparent-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proper-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_classes,\n",
    "                 model_name=model_name,\n",
    "                 use_fc=False,\n",
    "                 fc_dim=512,\n",
    "                 dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param n_classes:\n",
    "        :param model_name: name of model from pretrainedmodels\n",
    "            e.g. resnet50, resnext101_32x4d, pnasnet5large\n",
    "        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n",
    "        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n",
    "        \"\"\"\n",
    "        super(ShopeeNet, self).__init__()\n",
    "\n",
    "        self.bert_model= DistilBertTokenizer.from_pretrained(model_name) #TOKENIZER\n",
    "        final_in_features = 768 #self.bert_model.config.hidden_size\n",
    "        \n",
    "#         self.transformer = transformers.AutoModel.from_pretrained(model_name)\n",
    "#         final_in_features = self.transformer.config.hidden_size\n",
    "        \n",
    "        self.use_fc = use_fc\n",
    "    \n",
    "        if use_fc: #사용안함\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "            self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            final_in_features = fc_dim\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids,attention_mask):\n",
    "        feature = self.extract_feat(input_ids,attention_mask)\n",
    "        return F.normalize(feature)\n",
    "\n",
    "    def extract_feat(self, input_ids,attention_mask):\n",
    "        x = self.bert_model(input_ids=input_ids,attention_mask=attention_mask)\n",
    "        \n",
    "        features = x[0]\n",
    "        features = features[:,0,:]\n",
    "\n",
    "        if self.use_fc:\n",
    "            features = self.dropout(features)\n",
    "            features = self.fc(features)\n",
    "            features = self.bn(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "genuine-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 11014,\n",
       " 'model_name': 'cahya/distilbert-base-indonesian',\n",
       " 'use_fc': False,\n",
       " 'dropout': 0.3}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "skilled-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShopeeNet(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "southeast-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(df):\n",
    "    embeds = []\n",
    "    \n",
    "    model = ShopeeNet(**model_params)\n",
    "    model.eval()\n",
    "    \n",
    "    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1])) #여기서 에러\n",
    "    model = model.to(device)\n",
    "\n",
    "    text_dataset = ShopeeDataset(df)\n",
    "    text_loader = torch.utils.data.DataLoader(\n",
    "        text_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(text_loader): \n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "            feat = model(input_ids, attention_mask)\n",
    "            text_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(text_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    text_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our text embeddings shape is {text_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "regulation-prairie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>train_129225211 train_2278313361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>train_3386243561 train_3423213080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>train_2288590299 train_3803689425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>train_2406599165 train_3342059966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>train_3369186413 train_921438619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                             matches  \n",
       "0   train_129225211 train_2278313361  \n",
       "1  train_3386243561 train_3423213080  \n",
       "2  train_2288590299 train_3803689425  \n",
       "3  train_2406599165 train_3342059966  \n",
       "4   train_3369186413 train_921438619  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df,df_cu = read_dataset()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-intro",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "concrete-finance",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ShopeeNet:\n\tUnexpected key(s) in state_dict: \"bert_model.embeddings.word_embeddings.weight\", \"bert_model.embeddings.position_embeddings.weight\", \"bert_model.embeddings.LayerNorm.weight\", \"bert_model.embeddings.LayerNorm.bias\", \"bert_model.transformer.layer.0.attention.q_lin.weight\", \"bert_model.transformer.layer.0.attention.q_lin.bias\", \"bert_model.transformer.layer.0.attention.k_lin.weight\", \"bert_model.transformer.layer.0.attention.k_lin.bias\", \"bert_model.transformer.layer.0.attention.v_lin.weight\", \"bert_model.transformer.layer.0.attention.v_lin.bias\", \"bert_model.transformer.layer.0.attention.out_lin.weight\", \"bert_model.transformer.layer.0.attention.out_lin.bias\", \"bert_model.transformer.layer.0.sa_layer_norm.weight\", \"bert_model.transformer.layer.0.sa_layer_norm.bias\", \"bert_model.transformer.layer.0.ffn.lin1.weight\", \"bert_model.transformer.layer.0.ffn.lin1.bias\", \"bert_model.transformer.layer.0.ffn.lin2.weight\", \"bert_model.transformer.layer.0.ffn.lin2.bias\", \"bert_model.transformer.layer.0.output_layer_norm.weight\", \"bert_model.transformer.layer.0.output_layer_norm.bias\", \"bert_model.transformer.layer.1.attention.q_lin.weight\", \"bert_model.transformer.layer.1.attention.q_lin.bias\", \"bert_model.transformer.layer.1.attention.k_lin.weight\", \"bert_model.transformer.layer.1.attention.k_lin.bias\", \"bert_model.transformer.layer.1.attention.v_lin.weight\", \"bert_model.transformer.layer.1.attention.v_lin.bias\", \"bert_model.transformer.layer.1.attention.out_lin.weight\", \"bert_model.transformer.layer.1.attention.out_lin.bias\", \"bert_model.transformer.layer.1.sa_layer_norm.weight\", \"bert_model.transformer.layer.1.sa_layer_norm.bias\", \"bert_model.transformer.layer.1.ffn.lin1.weight\", \"bert_model.transformer.layer.1.ffn.lin1.bias\", \"bert_model.transformer.layer.1.ffn.lin2.weight\", \"bert_model.transformer.layer.1.ffn.lin2.bias\", \"bert_model.transformer.layer.1.output_layer_norm.weight\", \"bert_model.transformer.layer.1.output_layer_norm.bias\", \"bert_model.transformer.layer.2.attention.q_lin.weight\", \"bert_model.transformer.layer.2.attention.q_lin.bias\", \"bert_model.transformer.layer.2.attention.k_lin.weight\", \"bert_model.transformer.layer.2.attention.k_lin.bias\", \"bert_model.transformer.layer.2.attention.v_lin.weight\", \"bert_model.transformer.layer.2.attention.v_lin.bias\", \"bert_model.transformer.layer.2.attention.out_lin.weight\", \"bert_model.transformer.layer.2.attention.out_lin.bias\", \"bert_model.transformer.layer.2.sa_layer_norm.weight\", \"bert_model.transformer.layer.2.sa_layer_norm.bias\", \"bert_model.transformer.layer.2.ffn.lin1.weight\", \"bert_model.transformer.layer.2.ffn.lin1.bias\", \"bert_model.transformer.layer.2.ffn.lin2.weight\", \"bert_model.transformer.layer.2.ffn.lin2.bias\", \"bert_model.transformer.layer.2.output_layer_norm.weight\", \"bert_model.transformer.layer.2.output_layer_norm.bias\", \"bert_model.transformer.layer.3.attention.q_lin.weight\", \"bert_model.transformer.layer.3.attention.q_lin.bias\", \"bert_model.transformer.layer.3.attention.k_lin.weight\", \"bert_model.transformer.layer.3.attention.k_lin.bias\", \"bert_model.transformer.layer.3.attention.v_lin.weight\", \"bert_model.transformer.layer.3.attention.v_lin.bias\", \"bert_model.transformer.layer.3.attention.out_lin.weight\", \"bert_model.transformer.layer.3.attention.out_lin.bias\", \"bert_model.transformer.layer.3.sa_layer_norm.weight\", \"bert_model.transformer.layer.3.sa_layer_norm.bias\", \"bert_model.transformer.layer.3.ffn.lin1.weight\", \"bert_model.transformer.layer.3.ffn.lin1.bias\", \"bert_model.transformer.layer.3.ffn.lin2.weight\", \"bert_model.transformer.layer.3.ffn.lin2.bias\", \"bert_model.transformer.layer.3.output_layer_norm.weight\", \"bert_model.transformer.layer.3.output_layer_norm.bias\", \"bert_model.transformer.layer.4.attention.q_lin.weight\", \"bert_model.transformer.layer.4.attention.q_lin.bias\", \"bert_model.transformer.layer.4.attention.k_lin.weight\", \"bert_model.transformer.layer.4.attention.k_lin.bias\", \"bert_model.transformer.layer.4.attention.v_lin.weight\", \"bert_model.transformer.layer.4.attention.v_lin.bias\", \"bert_model.transformer.layer.4.attention.out_lin.weight\", \"bert_model.transformer.layer.4.attention.out_lin.bias\", \"bert_model.transformer.layer.4.sa_layer_norm.weight\", \"bert_model.transformer.layer.4.sa_layer_norm.bias\", \"bert_model.transformer.layer.4.ffn.lin1.weight\", \"bert_model.transformer.layer.4.ffn.lin1.bias\", \"bert_model.transformer.layer.4.ffn.lin2.weight\", \"bert_model.transformer.layer.4.ffn.lin2.bias\", \"bert_model.transformer.layer.4.output_layer_norm.weight\", \"bert_model.transformer.layer.4.output_layer_norm.bias\", \"bert_model.transformer.layer.5.attention.q_lin.weight\", \"bert_model.transformer.layer.5.attention.q_lin.bias\", \"bert_model.transformer.layer.5.attention.k_lin.weight\", \"bert_model.transformer.layer.5.attention.k_lin.bias\", \"bert_model.transformer.layer.5.attention.v_lin.weight\", \"bert_model.transformer.layer.5.attention.v_lin.bias\", \"bert_model.transformer.layer.5.attention.out_lin.weight\", \"bert_model.transformer.layer.5.attention.out_lin.bias\", \"bert_model.transformer.layer.5.sa_layer_norm.weight\", \"bert_model.transformer.layer.5.sa_layer_norm.bias\", \"bert_model.transformer.layer.5.ffn.lin1.weight\", \"bert_model.transformer.layer.5.ffn.lin1.bias\", \"bert_model.transformer.layer.5.ffn.lin2.weight\", \"bert_model.transformer.layer.5.ffn.lin2.bias\", \"bert_model.transformer.layer.5.output_layer_norm.weight\", \"bert_model.transformer.layer.5.output_layer_norm.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6f8a0e8beab6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_text_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-71684a384b7d>\u001b[0m in \u001b[0;36mget_text_embeddings\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#여기서 에러\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rapids-0.18/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ShopeeNet:\n\tUnexpected key(s) in state_dict: \"bert_model.embeddings.word_embeddings.weight\", \"bert_model.embeddings.position_embeddings.weight\", \"bert_model.embeddings.LayerNorm.weight\", \"bert_model.embeddings.LayerNorm.bias\", \"bert_model.transformer.layer.0.attention.q_lin.weight\", \"bert_model.transformer.layer.0.attention.q_lin.bias\", \"bert_model.transformer.layer.0.attention.k_lin.weight\", \"bert_model.transformer.layer.0.attention.k_lin.bias\", \"bert_model.transformer.layer.0.attention.v_lin.weight\", \"bert_model.transformer.layer.0.attention.v_lin.bias\", \"bert_model.transformer.layer.0.attention.out_lin.weight\", \"bert_model.transformer.layer.0.attention.out_lin.bias\", \"bert_model.transformer.layer.0.sa_layer_norm.weight\", \"bert_model.transformer.layer.0.sa_layer_norm.bias\", \"bert_model.transformer.layer.0.ffn.lin1.weight\", \"bert_model.transformer.layer.0.ffn.lin1.bias\", \"bert_model.transformer.layer.0.ffn.lin2.weight\", \"bert_model.transformer.layer.0.ffn.lin2.bias\", \"bert_model.transformer.layer.0.output_layer_norm.weight\", \"bert_model.transformer.layer.0.output_layer_norm.bias\", \"bert_model.transformer.layer.1.attention.q_lin.weight\", \"bert_model.transformer.layer.1.attention.q_lin.bias\", \"bert_model.transformer.layer.1.attention.k_lin.weight\", \"bert_model.transformer.layer.1.attention.k_lin.bias\", \"bert_model.transformer.layer.1.attention.v_lin.weight\", \"bert_model.transformer.layer.1.attention.v_lin.bias\", \"bert_model.transformer.layer.1.attention.out_lin.weight\", \"bert_model.transformer.layer.1.attention.out_lin.bias\", \"bert_model.transformer.layer.1.sa_layer_norm.weight\", \"bert_model.transformer.layer.1.sa_layer_norm.bias\", \"bert_model.transformer.layer.1.ffn.lin1.weight\", \"bert_model.transformer.layer.1.ffn.lin1.bias\", \"bert_model.transformer.layer.1.ffn.lin2.weight\", \"bert_model.transformer.layer.1.ffn.lin2.bias\", \"bert_model.transformer.layer.1.output_layer_norm.weight\", \"bert_model.transformer.layer.1.output_layer_norm.bias\", \"bert_model.transformer.layer.2.attention.q_lin.weight\", \"bert_model.transformer.layer.2.attention.q_lin.bias\", \"bert_model.transformer.layer.2.attention.k_lin.weight\", \"bert_model.transformer.layer.2.attention.k_lin.bias\", \"bert_model.transformer.layer.2.attention.v_lin.weight\", \"bert_model.transformer.layer.2.attention.v_lin.bias\", \"bert_model.transformer.layer.2.attention.out_lin.weight\", \"bert_model.transformer.layer.2.attention.out_lin.bias\", \"bert_model.transformer.layer.2.sa_layer_norm.weight\", \"bert_model.transformer.layer.2.sa_layer_norm.bias\", \"bert_model.transformer.layer.2.ffn.lin1.weight\", \"bert_model.transformer.layer.2.ffn.lin1.bias\", \"bert_model.transformer.layer.2.ffn.lin2.weight\", \"bert_model.transformer.layer.2.ffn.lin2.bias\", \"bert_model.transformer.layer.2.output_layer_norm.weight\", \"bert_model.transformer.layer.2.output_layer_norm.bias\", \"bert_model.transformer.layer.3.attention.q_lin.weight\", \"bert_model.transformer.layer.3.attention.q_lin.bias\", \"bert_model.transformer.layer.3.attention.k_lin.weight\", \"bert_model.transformer.layer.3.attention.k_lin.bias\", \"bert_model.transformer.layer.3.attention.v_lin.weight\", \"bert_model.transformer.layer.3.attention.v_lin.bias\", \"bert_model.transformer.layer.3.attention.out_lin.weight\", \"bert_model.transformer.layer.3.attention.out_lin.bias\", \"bert_model.transformer.layer.3.sa_layer_norm.weight\", \"bert_model.transformer.layer.3.sa_layer_norm.bias\", \"bert_model.transformer.layer.3.ffn.lin1.weight\", \"bert_model.transformer.layer.3.ffn.lin1.bias\", \"bert_model.transformer.layer.3.ffn.lin2.weight\", \"bert_model.transformer.layer.3.ffn.lin2.bias\", \"bert_model.transformer.layer.3.output_layer_norm.weight\", \"bert_model.transformer.layer.3.output_layer_norm.bias\", \"bert_model.transformer.layer.4.attention.q_lin.weight\", \"bert_model.transformer.layer.4.attention.q_lin.bias\", \"bert_model.transformer.layer.4.attention.k_lin.weight\", \"bert_model.transformer.layer.4.attention.k_lin.bias\", \"bert_model.transformer.layer.4.attention.v_lin.weight\", \"bert_model.transformer.layer.4.attention.v_lin.bias\", \"bert_model.transformer.layer.4.attention.out_lin.weight\", \"bert_model.transformer.layer.4.attention.out_lin.bias\", \"bert_model.transformer.layer.4.sa_layer_norm.weight\", \"bert_model.transformer.layer.4.sa_layer_norm.bias\", \"bert_model.transformer.layer.4.ffn.lin1.weight\", \"bert_model.transformer.layer.4.ffn.lin1.bias\", \"bert_model.transformer.layer.4.ffn.lin2.weight\", \"bert_model.transformer.layer.4.ffn.lin2.bias\", \"bert_model.transformer.layer.4.output_layer_norm.weight\", \"bert_model.transformer.layer.4.output_layer_norm.bias\", \"bert_model.transformer.layer.5.attention.q_lin.weight\", \"bert_model.transformer.layer.5.attention.q_lin.bias\", \"bert_model.transformer.layer.5.attention.k_lin.weight\", \"bert_model.transformer.layer.5.attention.k_lin.bias\", \"bert_model.transformer.layer.5.attention.v_lin.weight\", \"bert_model.transformer.layer.5.attention.v_lin.bias\", \"bert_model.transformer.layer.5.attention.out_lin.weight\", \"bert_model.transformer.layer.5.attention.out_lin.bias\", \"bert_model.transformer.layer.5.sa_layer_norm.weight\", \"bert_model.transformer.layer.5.sa_layer_norm.bias\", \"bert_model.transformer.layer.5.ffn.lin1.weight\", \"bert_model.transformer.layer.5.ffn.lin1.bias\", \"bert_model.transformer.layer.5.ffn.lin2.weight\", \"bert_model.transformer.layer.5.ffn.lin2.bias\", \"bert_model.transformer.layer.5.output_layer_norm.weight\", \"bert_model.transformer.layer.5.output_layer_norm.bias\". "
     ]
    }
   ],
   "source": [
    "text_embeddings = get_text_embeddings(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-utilization",
   "metadata": {},
   "source": [
    "## 여기서부터 custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.copy()\n",
    "test_dataset = test_df['title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colonial-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='cahya/distilbert-base-indonesian'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "bert_model = DistilBertModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tokenizer(test_dataset, padding=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "failing-stand",
   "metadata": {},
   "source": [
    "output = bert_model(input_ids=torch.tensor(batch['input_ids']), attention_mask=torch.tensor(batch['attention_mask']))\n",
    "last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "stone-affiliation",
   "metadata": {},
   "source": [
    "pretrained_CLS = last_hidden_state[:, 0, :]\n",
    "pretrained_CLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = '/mnt/hdd1/wearly/ethan/shopee/Train/weights/model_100epochs.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_state_dict = dict()\n",
    "\n",
    "for k,v in torch.load(pretrained_model_path).items() : \n",
    "    key = k.replace(\"bert_model.\",'')\n",
    "    revised_state_dict[key] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.load_state_dict(revised_state_dict, strict=False)\n",
    "bert_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-document",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShopeeDataset(Dataset):\n",
    "    def __init__(self, batch):\n",
    "        self.batch = batch\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tmp_ids = torch.tensor(self.batch['input_ids'])[idx]\n",
    "        tmp_mask = torch.tensor(self.batch['attention_mask'])[idx]\n",
    "        \n",
    "        return tmp_ids, tmp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_tensor = torch.tensor(batch['input_ids'])\n",
    "mask_tensor = torch.tensor(batch['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = []\n",
    "batch_size = 10\n",
    "\n",
    "for batch_idx in tqdm_notebook(range(0, df.shape[0], batch_size)) : \n",
    "    batch_id = id_tensor[batch_idx:batch_idx+batch_size].to(device)\n",
    "    batch_mask = mask_tensor[batch_idx:batch_idx+batch_size].to(device)\n",
    "    \n",
    "    output = bert_model(input_ids=batch_id, attention_mask=batch_mask)\n",
    "    last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n",
    "    mem.append(last_hidden_state.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.tensor(np.vstack(mem))\n",
    "embeddings = embeddings[:,0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-house",
   "metadata": {},
   "source": [
    "### pre-trained performance ( did not anything...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_neighbors_knn(test_df, embeddings.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-feature",
   "metadata": {},
   "source": [
    "### fine-tuned performance (arcface metric-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-services",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_ = get_neighbors_knn(test_df, embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-enforcement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
