{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017793,
     "end_time": "2021-02-18T14:09:55.551534",
     "exception": false,
     "start_time": "2021-02-18T14:09:55.533741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "b4_ns 에서 0.9 스코어를 찍었던 weights 를 가져와서 앙상블을 하였습니다.\n",
    "\n",
    "파일명은 다르기때문에 원코드의 형식에 맞게 임의로 맞춰주었슴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:55.593761Z",
     "iopub.status.busy": "2021-02-18T14:09:55.593151Z",
     "iopub.status.idle": "2021-02-18T14:09:57.073718Z",
     "shell.execute_reply": "2021-02-18T14:09:57.072687Z"
    },
    "papermill": {
     "duration": 1.506372,
     "end_time": "2021-02-18T14:09:57.073972",
     "exception": false,
     "start_time": "2021-02-18T14:09:55.567600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "package_path = '../input/vision-transformer-pytorch/VisionTransformer-Pytorch'\n",
    "sys.path.append(package_path)\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "\n",
    "package_path = '../input/timm-pytorch-image-models/pytorch-image-models-master'\n",
    "sys.path.append(package_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:57.113501Z",
     "iopub.status.busy": "2021-02-18T14:09:57.112840Z",
     "iopub.status.idle": "2021-02-18T14:09:59.602535Z",
     "shell.execute_reply": "2021-02-18T14:09:59.601422Z"
    },
    "papermill": {
     "duration": 2.511882,
     "end_time": "2021-02-18T14:09:59.602710",
     "exception": false,
     "start_time": "2021-02-18T14:09:57.090828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "# import timm #from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:59.646546Z",
     "iopub.status.busy": "2021-02-18T14:09:59.644812Z",
     "iopub.status.idle": "2021-02-18T14:09:59.647193Z",
     "shell.execute_reply": "2021-02-18T14:09:59.647593Z"
    },
    "papermill": {
     "duration": 0.025812,
     "end_time": "2021-02-18T14:09:59.647718",
     "exception": false,
     "start_time": "2021-02-18T14:09:59.621906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 10,\n",
    "    'seed': 1109, #1109,#329,#225, #719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 384,\n",
    "    'epochs': 32,\n",
    "    'train_bs': 28,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'used_epochs': [1,2],\n",
    "    'weights': [1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:59.684475Z",
     "iopub.status.busy": "2021-02-18T14:09:59.683977Z",
     "iopub.status.idle": "2021-02-18T14:09:59.714593Z",
     "shell.execute_reply": "2021-02-18T14:09:59.713645Z"
    },
    "papermill": {
     "duration": 0.05087,
     "end_time": "2021-02-18T14:09:59.714700",
     "exception": false,
     "start_time": "2021-02-18T14:09:59.663830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:59.753918Z",
     "iopub.status.busy": "2021-02-18T14:09:59.752277Z",
     "iopub.status.idle": "2021-02-18T14:09:59.754535Z",
     "shell.execute_reply": "2021-02-18T14:09:59.754960Z"
    },
    "papermill": {
     "duration": 0.024067,
     "end_time": "2021-02-18T14:09:59.755080",
     "exception": false,
     "start_time": "2021-02-18T14:09:59.731013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:59.794612Z",
     "iopub.status.busy": "2021-02-18T14:09:59.794112Z",
     "iopub.status.idle": "2021-02-18T14:09:59.797729Z",
     "shell.execute_reply": "2021-02-18T14:09:59.797324Z"
    },
    "papermill": {
     "duration": 0.026521,
     "end_time": "2021-02-18T14:09:59.797847",
     "exception": false,
     "start_time": "2021-02-18T14:09:59.771326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:09:59.845006Z",
     "iopub.status.busy": "2021-02-18T14:09:59.844359Z",
     "iopub.status.idle": "2021-02-18T14:10:00.735390Z",
     "shell.execute_reply": "2021-02-18T14:10:00.736414Z"
    },
    "papermill": {
     "duration": 0.921478,
     "end_time": "2021-02-18T14:10:00.736608",
     "exception": false,
     "start_time": "2021-02-18T14:09:59.815130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:10:00.797062Z",
     "iopub.status.busy": "2021-02-18T14:10:00.796411Z",
     "iopub.status.idle": "2021-02-18T14:10:00.801436Z",
     "shell.execute_reply": "2021-02-18T14:10:00.802396Z"
    },
    "papermill": {
     "duration": 0.041876,
     "end_time": "2021-02-18T14:10:00.804752",
     "exception": false,
     "start_time": "2021-02-18T14:10:00.762876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class EnsembleClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model1 = VisionTransformer.from_name('ViT-B_16', num_classes=5) \n",
    "        self.model1.load_state_dict(torch.load('../input/vit-model-1/ViT-B_16.pt'))\n",
    "        self.model2 = CassvaImgClassifier(model_arch, n_class, pretrained)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        x2 = self.model2(x)\n",
    "        return 0.7 * x1 + 0.3 * x2\n",
    "        \n",
    "    \n",
    "    def load(self, state_dict):\n",
    "        self.model2.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:10:00.860518Z",
     "iopub.status.busy": "2021-02-18T14:10:00.858839Z",
     "iopub.status.idle": "2021-02-18T14:10:00.861130Z",
     "shell.execute_reply": "2021-02-18T14:10:00.861527Z"
    },
    "papermill": {
     "duration": 0.028755,
     "end_time": "2021-02-18T14:10:00.861648",
     "exception": false,
     "start_time": "2021-02-18T14:10:00.832893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:10:00.908116Z",
     "iopub.status.busy": "2021-02-18T14:10:00.899609Z",
     "iopub.status.idle": "2021-02-18T14:16:13.795144Z",
     "shell.execute_reply": "2021-02-18T14:16:13.795799Z"
    },
    "papermill": {
     "duration": 372.917587,
     "end_time": "2021-02-18T14:16:13.796008",
     "exception": false,
     "start_time": "2021-02-18T14:10:00.878421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [01:01<00:00,  1.09it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.48it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.15it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.77it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.44it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.26440\n",
      "fold 0 validation accuracy = 0.91776\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "        if fold > 0:\n",
    "            break \n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        tst_loader = torch.utils.data.DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "        model = EnsembleClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        \n",
    "        val_preds = []\n",
    "        tst_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):\n",
    "        for i, epoch in enumerate(CFG['used_epochs']):    \n",
    "            model.load(torch.load('../input/best-b4s/{}_fold_{}_{}.pth'.format(CFG['model_arch'], fold, epoch)))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "                    tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n",
    "\n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        tst_preds = np.mean(tst_preds, axis=0) \n",
    "        \n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:14.054803Z",
     "iopub.status.busy": "2021-02-18T14:16:14.054111Z",
     "iopub.status.idle": "2021-02-18T14:16:14.056949Z",
     "shell.execute_reply": "2021-02-18T14:16:14.056496Z"
    },
    "papermill": {
     "duration": 0.132883,
     "end_time": "2021-02-18T14:16:14.057068",
     "exception": false,
     "start_time": "2021-02-18T14:16:13.924185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test['label'] = np.argmax(tst_preds, axis=1)\n",
    "#test.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:14.525663Z",
     "iopub.status.busy": "2021-02-18T14:16:14.524872Z",
     "iopub.status.idle": "2021-02-18T14:16:14.540761Z",
     "shell.execute_reply": "2021-02-18T14:16:14.541771Z"
    },
    "papermill": {
     "duration": 0.268161,
     "end_time": "2021-02-18T14:16:14.542007",
     "exception": false,
     "start_time": "2021-02-18T14:16:14.273846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['df_test_predict_proba_1', 'variable_list']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from scipy.special import softmax\n",
    "\n",
    "df_test_predict_proba_1 = pd.concat(\n",
    "    [test, pd.DataFrame(softmax(tst_preds, axis = 1))],\n",
    "    axis=1\n",
    ").sort_values([\"image_id\"])\n",
    "\n",
    "variable_list = %who_ls\n",
    "for _ in variable_list:\n",
    "    if _ is not \"df_test_predict_proba_1\":\n",
    "        del globals()[_]\n",
    "\n",
    "%who_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.126832,
     "end_time": "2021-02-18T14:16:14.860257",
     "exception": false,
     "start_time": "2021-02-18T14:16:14.733425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:15.123218Z",
     "iopub.status.busy": "2021-02-18T14:16:15.122487Z",
     "iopub.status.idle": "2021-02-18T14:16:15.125479Z",
     "shell.execute_reply": "2021-02-18T14:16:15.125078Z"
    },
    "papermill": {
     "duration": 0.138108,
     "end_time": "2021-02-18T14:16:15.125590",
     "exception": false,
     "start_time": "2021-02-18T14:16:14.987482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "package_path = '../input/vision-transformer-pytorch/VisionTransformer-Pytorch'\n",
    "sys.path.append(package_path)\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "\n",
    "package_path = '../input/timm-pytorch-image-models/pytorch-image-models-master'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "# import timm #from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:15.384953Z",
     "iopub.status.busy": "2021-02-18T14:16:15.384326Z",
     "iopub.status.idle": "2021-02-18T14:16:15.387146Z",
     "shell.execute_reply": "2021-02-18T14:16:15.386715Z"
    },
    "papermill": {
     "duration": 0.134317,
     "end_time": "2021-02-18T14:16:15.387254",
     "exception": false,
     "start_time": "2021-02-18T14:16:15.252937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 10,\n",
    "    'seed': 1109, #1109,#329,#225, #719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 384,\n",
    "    'epochs': 32,\n",
    "    'train_bs': 28,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'used_epochs': [1,2],\n",
    "    'weights': [1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:15.648136Z",
     "iopub.status.busy": "2021-02-18T14:16:15.647251Z",
     "iopub.status.idle": "2021-02-18T14:16:15.705974Z",
     "shell.execute_reply": "2021-02-18T14:16:15.705470Z"
    },
    "papermill": {
     "duration": 0.192479,
     "end_time": "2021-02-18T14:16:15.706087",
     "exception": false,
     "start_time": "2021-02-18T14:16:15.513608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class EnsembleClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model1 = VisionTransformer.from_name('ViT-B_16', num_classes=5) \n",
    "        self.model1.load_state_dict(torch.load('../input/vit-model-1/ViT-B_16.pt'))\n",
    "        self.model2 = CassvaImgClassifier(model_arch, n_class, pretrained)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        x2 = self.model2(x)\n",
    "        return x2\n",
    "    \n",
    "    def load(self, state_dict):\n",
    "        self.model2.load_state_dict(state_dict)\n",
    "        \n",
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:16:15.964684Z",
     "iopub.status.busy": "2021-02-18T14:16:15.963884Z",
     "iopub.status.idle": "2021-02-18T14:22:13.887803Z",
     "shell.execute_reply": "2021-02-18T14:22:13.888445Z"
    },
    "papermill": {
     "duration": 358.0548,
     "end_time": "2021-02-18T14:22:13.888660",
     "exception": false,
     "start_time": "2021-02-18T14:16:15.833860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.44it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.77it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.52it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.82it/s]\n",
      "100%|██████████| 67/67 [00:59<00:00,  1.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  7.08it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.29107\n",
      "fold 0 validation accuracy = 0.92477\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "        if fold > 0:\n",
    "            break \n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        tst_loader = torch.utils.data.DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "        model = EnsembleClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        \n",
    "        val_preds = []\n",
    "        tst_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):\n",
    "        for i, epoch in enumerate(CFG['used_epochs']):    \n",
    "            model.load(torch.load('../input/best-b4s/{}_fold_{}_{}.pth'.format(CFG['model_arch'], fold, epoch)))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "                    tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n",
    "\n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        tst_preds = np.mean(tst_preds, axis=0) \n",
    "        \n",
    "        nep_val_loss = log_loss(valid_.label.values, val_preds)\n",
    "        nep_val_acc = (valid_.label.values==np.argmax(val_preds, axis=1)).mean()\n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:22:14.370151Z",
     "iopub.status.busy": "2021-02-18T14:22:14.369474Z",
     "iopub.status.idle": "2021-02-18T14:22:14.373245Z",
     "shell.execute_reply": "2021-02-18T14:22:14.372849Z"
    },
    "papermill": {
     "duration": 0.247021,
     "end_time": "2021-02-18T14:22:14.373354",
     "exception": false,
     "start_time": "2021-02-18T14:22:14.126333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "df_test_predict_proba_2 = pd.concat([test, pd.DataFrame(softmax(tst_preds, axis = 1))],axis=1).sort_values([\"image_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.276098,
     "end_time": "2021-02-18T14:22:14.887830",
     "exception": false,
     "start_time": "2021-02-18T14:22:14.611732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3rd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:22:15.381229Z",
     "iopub.status.busy": "2021-02-18T14:22:15.379481Z",
     "iopub.status.idle": "2021-02-18T14:22:15.381855Z",
     "shell.execute_reply": "2021-02-18T14:22:15.382262Z"
    },
    "papermill": {
     "duration": 0.25215,
     "end_time": "2021-02-18T14:22:15.382393",
     "exception": false,
     "start_time": "2021-02-18T14:22:15.130243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "package_path = '../input/vision-transformer-pytorch/VisionTransformer-Pytorch'\n",
    "sys.path.append(package_path)\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "\n",
    "package_path = '../input/timm-pytorch-image-models/pytorch-image-models-master'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "# import timm #from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:22:15.869385Z",
     "iopub.status.busy": "2021-02-18T14:22:15.868644Z",
     "iopub.status.idle": "2021-02-18T14:22:15.871542Z",
     "shell.execute_reply": "2021-02-18T14:22:15.871119Z"
    },
    "papermill": {
     "duration": 0.249201,
     "end_time": "2021-02-18T14:22:15.871661",
     "exception": false,
     "start_time": "2021-02-18T14:22:15.622460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'fold_num': 10,\n",
    "    'seed': 319, #1109,#329,#225, #719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 384,\n",
    "    'epochs': 32,\n",
    "    'train_bs': 28,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'used_epochs': [3,4],\n",
    "    'weights': [1,1,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:22:16.384353Z",
     "iopub.status.busy": "2021-02-18T14:22:16.383534Z",
     "iopub.status.idle": "2021-02-18T14:22:16.418410Z",
     "shell.execute_reply": "2021-02-18T14:22:16.417995Z"
    },
    "papermill": {
     "duration": 0.302016,
     "end_time": "2021-02-18T14:22:16.418531",
     "exception": false,
     "start_time": "2021-02-18T14:22:16.116515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class EnsembleClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model1 = VisionTransformer.from_name('ViT-B_16', num_classes=5) \n",
    "        self.model1.load_state_dict(torch.load('../input/vit-model-1/ViT-B_16.pt'))\n",
    "        self.model2 = CassvaImgClassifier(model_arch, n_class, pretrained)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        x2 = self.model2(x)\n",
    "        return 0.7 * x1 + 0.3 * x2\n",
    "    \n",
    "    def load(self, state_dict):\n",
    "        self.model2.load_state_dict(state_dict)\n",
    "        \n",
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:22:16.955506Z",
     "iopub.status.busy": "2021-02-18T14:22:16.954700Z",
     "iopub.status.idle": "2021-02-18T14:28:18.222845Z",
     "shell.execute_reply": "2021-02-18T14:28:18.222325Z"
    },
    "papermill": {
     "duration": 361.563736,
     "end_time": "2021-02-18T14:28:18.222987",
     "exception": false,
     "start_time": "2021-02-18T14:22:16.659251",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.76it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.87it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.50it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.26827\n",
      "fold 0 validation accuracy = 0.91308\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "        if fold > 0:\n",
    "            break \n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        tst_loader = torch.utils.data.DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "        model = EnsembleClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        \n",
    "        val_preds = []\n",
    "        tst_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):\n",
    "        for i, epoch in enumerate(CFG['used_epochs']):    \n",
    "            model.load(torch.load('../input/seed1234/{}_fold_{}_{}.pth'.format(CFG['model_arch'], fold, epoch)))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "                    tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n",
    "\n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        tst_preds = np.mean(tst_preds, axis=0) \n",
    "        \n",
    "        nep_val_loss = log_loss(valid_.label.values, val_preds)\n",
    "        nep_val_acc = (valid_.label.values==np.argmax(val_preds, axis=1)).mean()\n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:28:18.929123Z",
     "iopub.status.busy": "2021-02-18T14:28:18.928438Z",
     "iopub.status.idle": "2021-02-18T14:28:18.931733Z",
     "shell.execute_reply": "2021-02-18T14:28:18.932253Z"
    },
    "papermill": {
     "duration": 0.361401,
     "end_time": "2021-02-18T14:28:18.932396",
     "exception": false,
     "start_time": "2021-02-18T14:28:18.570995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "df_test_predict_proba_3 = pd.concat([test, pd.DataFrame(softmax(tst_preds, axis = 1))],axis=1).sort_values([\"image_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:28:19.713029Z",
     "iopub.status.busy": "2021-02-18T14:28:19.664163Z",
     "iopub.status.idle": "2021-02-18T14:28:19.730898Z",
     "shell.execute_reply": "2021-02-18T14:28:19.730091Z"
    },
    "papermill": {
     "duration": 0.434313,
     "end_time": "2021-02-18T14:28:19.731021",
     "exception": false,
     "start_time": "2021-02-18T14:28:19.296708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "package_path = '../input/vision-transformer-pytorch/VisionTransformer-Pytorch'\n",
    "sys.path.append(package_path)\n",
    "from vision_transformer_pytorch import VisionTransformer\n",
    "\n",
    "package_path = '../input/timm-pytorch-image-models/pytorch-image-models-master'\n",
    "sys.path.append(package_path)\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "import cv2\n",
    "from skimage import io\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import cv2\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler, RandomSampler\n",
    "from  torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "import sklearn\n",
    "import warnings\n",
    "import joblib\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn import metrics\n",
    "import warnings\n",
    "import cv2\n",
    "import pydicom\n",
    "# import timm #from efficientnet_pytorch import EfficientNet\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "CFG = {\n",
    "    'fold_num': 10,\n",
    "    'seed': 319, #1109,#329,#225, #719,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'img_size': 384,\n",
    "    'epochs': 32,\n",
    "    'train_bs': 28,\n",
    "    'valid_bs': 32,\n",
    "    'lr': 1e-4,\n",
    "    'num_workers': 4,\n",
    "    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'tta': 3,\n",
    "    'used_epochs': [3,4],\n",
    "    'weights': [1,1,1,1]\n",
    "}\n",
    "\n",
    "train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n",
    "submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    #print(im_rgb)\n",
    "    return im_rgb\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, df, data_root, transforms=None, output_label=True\n",
    "    ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.transforms = transforms\n",
    "        self.data_root = data_root\n",
    "        self.output_label = output_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index: int):\n",
    "        \n",
    "        # get labels\n",
    "        if self.output_label:\n",
    "            target = self.df.iloc[index]['label']\n",
    "          \n",
    "        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n",
    "        \n",
    "        img  = get_img(path)\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "            \n",
    "        # do label smoothing\n",
    "        if self.output_label == True:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img\n",
    "        \n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            ShiftScaleRotate(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            CoarseDropout(p=0.5),\n",
    "            Cutout(p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "  \n",
    "        \n",
    "def get_valid_transforms():\n",
    "    return Compose([\n",
    "            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n",
    "            Resize(CFG['img_size'], CFG['img_size']),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "def get_inference_transforms():\n",
    "    return Compose([\n",
    "            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "            Transpose(p=0.5),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.)\n",
    "\n",
    "class CassvaImgClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.model = timm.create_model(model_arch, pretrained=pretrained)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, n_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "class EnsembleClassifier(nn.Module):\n",
    "    def __init__(self, model_arch, n_class, pretrained=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model1 = VisionTransformer.from_name('ViT-B_16', num_classes=5) \n",
    "        self.model1.load_state_dict(torch.load('../input/vit-model-1/ViT-B_16.pt'))\n",
    "        self.model2 = CassvaImgClassifier(model_arch, n_class, pretrained)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.model1(x)\n",
    "        x2 = self.model2(x)\n",
    "        return x2\n",
    "    \n",
    "    def load(self, state_dict):\n",
    "        self.model2.load_state_dict(state_dict)\n",
    "        \n",
    "def inference_one_epoch(model, data_loader, device):\n",
    "    model.eval()\n",
    "\n",
    "    image_preds_all = []\n",
    "    \n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for step, (imgs) in pbar:\n",
    "        imgs = imgs.to(device).float()\n",
    "        \n",
    "        image_preds = model(imgs)   #output = model(input)\n",
    "        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n",
    "        \n",
    "    \n",
    "    image_preds_all = np.concatenate(image_preds_all, axis=0)\n",
    "    return image_preds_all\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:28:20.706172Z",
     "iopub.status.busy": "2021-02-18T14:28:20.705315Z",
     "iopub.status.idle": "2021-02-18T14:34:17.919537Z",
     "shell.execute_reply": "2021-02-18T14:34:17.918898Z"
    },
    "papermill": {
     "duration": 357.829015,
     "end_time": "2021-02-18T14:34:17.919708",
     "exception": false,
     "start_time": "2021-02-18T14:28:20.090693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference fold 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.61it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n",
      "100%|██████████| 67/67 [00:59<00:00,  1.13it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.37it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.58it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.86it/s]\n",
      "100%|██████████| 67/67 [00:58<00:00,  1.14it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  6.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0 validation loss = 0.29797\n",
      "fold 0 validation accuracy = 0.91729\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "     # for training only, need nightly build pytorch\n",
    "\n",
    "    seed_everything(CFG['seed'])\n",
    "    \n",
    "    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n",
    "    \n",
    "    for fold, (trn_idx, val_idx) in enumerate(folds):\n",
    "        # we'll train fold 0 first\n",
    "        if fold > 0:\n",
    "            break \n",
    "\n",
    "        print('Inference fold {} started'.format(fold))\n",
    "\n",
    "        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n",
    "        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        test = pd.DataFrame()\n",
    "        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n",
    "        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n",
    "        \n",
    "        val_loader = torch.utils.data.DataLoader(\n",
    "            valid_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "        \n",
    "        tst_loader = torch.utils.data.DataLoader(\n",
    "            test_ds, \n",
    "            batch_size=CFG['valid_bs'],\n",
    "            num_workers=CFG['num_workers'],\n",
    "            shuffle=False,\n",
    "            pin_memory=False,\n",
    "        )\n",
    "\n",
    "        device = torch.device(CFG['device'])\n",
    "        model = EnsembleClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n",
    "        \n",
    "        val_preds = []\n",
    "        tst_preds = []\n",
    "        \n",
    "        #for epoch in range(CFG['epochs']-3):\n",
    "        for i, epoch in enumerate(CFG['used_epochs']):    \n",
    "            model.load(torch.load('../input/seed1234/{}_fold_{}_{}.pth'.format(CFG['model_arch'], fold, epoch)))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for _ in range(CFG['tta']):\n",
    "                    val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n",
    "                    tst_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n",
    "\n",
    "        val_preds = np.mean(val_preds, axis=0) \n",
    "        tst_preds = np.mean(tst_preds, axis=0) \n",
    "        \n",
    "        nep_val_loss = log_loss(valid_.label.values, val_preds)\n",
    "        nep_val_acc = (valid_.label.values==np.argmax(val_preds, axis=1)).mean()\n",
    "        print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n",
    "        print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n",
    "        \n",
    "        del model\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:34:18.900362Z",
     "iopub.status.busy": "2021-02-18T14:34:18.899536Z",
     "iopub.status.idle": "2021-02-18T14:34:18.903714Z",
     "shell.execute_reply": "2021-02-18T14:34:18.903051Z"
    },
    "papermill": {
     "duration": 0.487903,
     "end_time": "2021-02-18T14:34:18.903867",
     "exception": false,
     "start_time": "2021-02-18T14:34:18.415964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "df_test_predict_proba_4 = pd.concat([test, pd.DataFrame(softmax(tst_preds, axis = 1))],axis=1).sort_values([\"image_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.461347,
     "end_time": "2021-02-18T14:34:19.868573",
     "exception": false,
     "start_time": "2021-02-18T14:34:19.407226",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-18T14:34:20.798027Z",
     "iopub.status.busy": "2021-02-18T14:34:20.797254Z",
     "iopub.status.idle": "2021-02-18T14:34:21.083723Z",
     "shell.execute_reply": "2021-02-18T14:34:21.082357Z"
    },
    "papermill": {
     "duration": 0.756036,
     "end_time": "2021-02-18T14:34:21.083903",
     "exception": false,
     "start_time": "2021-02-18T14:34:20.327867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "submission = df_test_predict_proba_1[['image_id']]\n",
    "\n",
    "submission['label'] = (((df_test_predict_proba_1.drop(['image_id'],axis=1)*0.7 + \n",
    "                       df_test_predict_proba_2.drop(['image_id'],axis=1)*0.3))*0.7 +\n",
    "                      ((df_test_predict_proba_3.drop(['image_id'],axis=1)*0.7 + \n",
    "                       df_test_predict_proba_4.drop(['image_id'],axis=1)*0.3))*0.3).to_numpy().argmax(1)\n",
    "\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1472.301493,
   "end_time": "2021-02-18T14:34:22.757985",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-18T14:09:50.456492",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
